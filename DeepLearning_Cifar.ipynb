{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6DyphAUEBOi"
      },
      "source": [
        "# Neural Network exam {-}\n",
        "\n",
        "This assignment aims at familiarizing you with training and testing a Deep Neural Network (DNN). Here are th requirements of the assignment:\n",
        "\n",
        "- Load the data.\n",
        "- Process and normalize the images.\n",
        "- Build, train and test a model that has at most 4M (four millions) parameters (use model.summary() to check). The model must be built FROM SCRATCH and its architecture is of your choice.\n",
        "- Evaluate the model performance on the test set.\n",
        "\n",
        "The dataset you will be working on is cifar10 (https://www.cs.toronto.edu/~kriz/cifar.html) which consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. Here follows the ten object classes:\n",
        "* airplane\n",
        "*\tautomobile\n",
        "*\tbird\n",
        "*\tcat\n",
        "*\tdeer\n",
        "*\tdog\n",
        "*\tfrog\n",
        "*\thorse\n",
        "*\tship\n",
        "*\ttruck\n",
        "\n",
        "Here follows some data samples in the dataset:\n",
        "\n",
        "![alt text](https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png)\n",
        "\n",
        "### Submission {-}\n",
        "Coding environement:\n",
        "- Google Colab: https://colab.research.google.com/\n",
        "- Framework: Tensorflow\n",
        "\n",
        "The structure of submission folder should be organized as follows:\n",
        "\n",
        "- ./\\<StudentID>-<StudentName>-<Test-accuracy>.ipynb: Jupyter notebook containing source code. For example, if your studentID is 20C12345, your name is Nguyen Van A, and you get 0.8124 accuracy on the test set, the name of this file is 20C12345-Nguyen-Van-A-08124.ipynb.\n",
        "\n",
        "### Evaluation {-}\n",
        "Assignment evaluation will be conducted on how you accomplish the assignment requirements. In addition, your code should conform to a Python coding convention such as PEP-8. The grading schema is as follows:\n",
        "- Top 5 test test accuracy: 10 points.\n",
        "- Top 6-10 test accuray: 9 points.\n",
        "- Top 11-20 test accuracy: 8 points.\n",
        "- Top 21-last test accuracy: 7 points.\n",
        "- Do not meet the assignment requirements (model having at most 4M parameters and built from scratch): 4 points.\n",
        "- Plagiarism: 0 point.\n",
        "- Do not submit: 0 point.\n",
        "\n",
        "### Deadline {-}\n",
        "- Deadline: 15/09/2023\n",
        "- Submission link: https://docs.google.com/forms/d/18Arc4XRlH7qArurvELE4pzlKYS7IZfqoy1oxKuPcqa0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7yiMO-bEBOl"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras              # Keras is the high-level API of TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6_ZhoDnfROl",
        "outputId": "6cef1b10-51d2-4912-8f55-f55a2c243f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: X_train = (45000, 32, 32, 3), y_train = (45000, 1)\n",
            "Validation shape: X_val = (5000, 32, 32, 3), y_val = (5000, 1)\n",
            "Test shape: X_test = (10000, 32, 32, 3), y_test = (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "# PLEASE DO NOT CHANGE THIS CODE\n",
        "\n",
        "# Load the cifar10 dataset and split train/test\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Split train/valid from the training set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=5)\n",
        "\n",
        "print(\"Train shape: X_train = \" + str(X_train.shape) + \", y_train = \" + str(y_train.shape))\n",
        "print(\"Validation shape: X_val = \" + str(X_val.shape) + \", y_val = \" + str(y_val.shape))\n",
        "print(\"Test shape: X_test = \" + str(X_test.shape) + \", y_test = \" + str(y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "v9EqVmHqfoL9",
        "outputId": "4db2a2b2-454e-4bf5-ee58-9b8ccdb94152"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAu10lEQVR4nO3de3DV9Z3/8df3XHM/GC65lEABrWgR+itVmp+tS4UV2BlHK7OjbWcWu46ObnBW2W5bdlqt7u7EtTOtbcfizG+7sp1f0dadoqOz1SqW+OsW2IXKj9pLfkJRUEhUai7kcm7fz+8Pa7qpoJ83JHyS8HzMnBmSvHnn872dd07OOa9EzjknAADOsEToBQAAzk4MIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEKnQC/hjcRzryJEjqq2tVRRFoZcDADByzqm/v1/Nzc1KJE7+OGfCDaAjR46opaUl9DIAAKfp8OHDmj179km/Pm4D6P7779dXv/pVdXV1acmSJfrWt76lSy655D3/X21trSRp+fJLlUr5La+kpPe6koq9ayUpYXgQlq2sM/VOVVT418qWmJR0Be9aZ/xNbMF42lgeySZc2baWUtG/d8K27lQybai27cPYeDwt1Qnzbw78uxeN604Y6lsaZ5l6V1TUeNe6bNbU+9Ch/ab6/T/f7V2bSdvOwzcGhr1rfzfsf91LUspwTcTO/3qI47LefPnAyP35Sb+/d0eD73//+9qwYYMeeOABLVu2TPfdd59WrVqlzs5OzZr17ifZ23dWqVRKae8DNX4DKGm4ltNpyx2WlEpn/GuN604ZIv5i452nMw8g//4JV7KtxXB87API//hMqAFk+anJ3H38BlDWOCQs9c7ww54kpTOWYy8lk/73QSlDrbV39C6/7jqRd/v12Ds427ql9/7hc1xehPC1r31NN954oz772c/qwgsv1AMPPKCqqir9y7/8y3h8OwDAJDTmA6hQKGjPnj1auXLlH75JIqGVK1dqx44d76jP5/Pq6+sbdQMATH1jPoDeeOMNlctlNTQ0jPp8Q0ODurq63lHf3t6uXC43cuMFCABwdgj+PqCNGzeqt7d35Hb48OHQSwIAnAFj/iKEGTNmKJlMqru7e9Tnu7u71djY+I76bDZrfvIRADD5jfkjoEwmo6VLl2rbtm0jn4vjWNu2bVNra+tYfzsAwCQ1Li/D3rBhg9atW6ePfOQjuuSSS3TfffdpYGBAn/3sZ8fj2wEAJqFxGUDXXnutXn/9dd1xxx3q6urShz70IT355JPveGECAODsNW5JCOvXr9f69etP+f+nMhmlPN/YmUpXevfNJG2/dUwZ3qiVrppm6q20/5vjEtY3osZ571pneNOqJCmyPWcXJf1PM/MbhUv+7xJPpWzrzmSrvWudbG/+jGPbG24tbxhMZ2zbWY79j3/Z+EbhpOE8rJ8x09S7otJwfIz75NVXbfX+eRySjNdbwvCm2FTZ1rtU8F950ZA64mK/6zj4q+AAAGcnBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCIcYviOV0XL7tEFZ5/x3333t949y2XyqZ11NbU+Bcb4z5KSf+IDUtciiSlDZFDKdniVQqxX0TSH76Bf+RQlLBF8VSk/bczYViHJEWpKkO17We5tCGiRpKSqaR3bWSMHIqdf4xQomw7VzKR/7lSWV1r6l1R6R/BFaX8rzVJymZt58qAIaYmYYxhypf9j08ptt2/lT0jcySp5PxrnWctj4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQUzYLLjhQkEu4ZeBVCwb8o8MuUqSlE7676KEIZdMksqG2LO8ZRslZRL+9RXGs8AZ8/Qs6W4J2Y5PpSHzTklb7zjyr4+NP8ulUsbsOEO9fyrZW1zk3zuWfyadJKXS/nlt6Ywtfy2ZNlybhutYklLG7LiC5dQy5K9J0nDBv75ozOqT/DMmY+df6zxreQQEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhiwkbxdHT8H6VSfsuLkzXefasq/GslKWmIeimXbCEoQ8MF79p80RbfUVXpnw2SyqZNvWPjdrrIfzud8YzMZP2jYUrGCJRiyX/dcWRbeJS0xRmlIv/tLJRscSyWiBUVbetOZA1xOcZ4osgQrZQwxjAlDDFMkpQxxAgVjddPZIjVimLDsZQUR4bentFov6/2quIREAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCICZsFl05mlUr6LS9vyMmKbRFPSqT8c9KGh/Km3kOD/lljZVuMmcoZ/3VHCdtpEMe27YzKtuwri2RU7V1biG05ZmVLFpwx2y32P2UlSdmE/38YjmwnS6Lsnx+WjG05cylDployZcsx87x7kCQljPvEEAH5Vn/nf3ySxpy5VNJ/7VHJtnCnYf/eYx8FxyMgAEAYYz6AvvKVryiKolG3hQsXjvW3AQBMcuPyK7gPfvCDeuaZZ/7wTTz/rAIA4OwxLpMhlUqpsbFxPFoDAKaIcXkO6MUXX1Rzc7Pmz5+vz3zmMzp06NBJa/P5vPr6+kbdAABT35gPoGXLlmnz5s168skntWnTJh08eFAf//jH1d/ff8L69vZ25XK5kVtLS8tYLwkAMAGN+QBas2aN/vzP/1yLFy/WqlWr9O///u/q6enRD37wgxPWb9y4Ub29vSO3w4cPj/WSAAAT0Li/OmDatGn6wAc+oP3795/w69lsVtlsdryXAQCYYMb9fUDHjx/XgQMH1NTUNN7fCgAwiYz5APrc5z6njo4OvfTSS/rZz36mT37yk0omk/rUpz411t8KADCJjfmv4F555RV96lOf0rFjxzRz5kx97GMf086dOzVz5kxTnyiRVOQbP2JI8IiNUSKW5uXYFvdRLPpHvRRLtt6lSv/cDGeMBimVbbEzLjZE2kTG3jn/KJ5yyda7UPCvd5ZcGEmxsT6K/H9WdM64D53/NVEu266fKPKPhEoa829SCf967/uS34sN+1uSys5wfBL+++Stev/jmU4Zs8bkf79iOat87zXHfAA9/PDDY90SADAFkQUHAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhi3P8cw6kqOynyDBQql4vefUt5W06WDLlaxeKwqfXg4ID/MkydpTjOeNdGxiy4YtF/f0tSbMibUtK6pf6sOWb5vH+GnSHyTJIUZ415YIZdePz4cVPvgiEjL2OLJJRzFd61KWNeW9aSp5ew3dUlM7bjU1Fd611bsF0+Shjy3ZLFIVPvVMF/v1iuzLhcls+9G4+AAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBTNgoHqeEnOd8jAw5JemkLXYmkTAFUBh7+9eWY1tETWyIELJG8cjZ1pJI+m+oZZ9IUsK0duN2GqRStkspYYyGiSL/mJpEwradqbT/Tk/Fxusn6b9u3+ittw31+kcORRn/aCpJStlSgVRd5R85lC7ZmvveD0pSwRCtI0muyr82YbjvLJdKet2np/+3BwBg7DCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBTNgsuJqaGqXSaa/aN3v6vPtWVdgyoZKGXK2kMUCqpsY/iKlQ8s92kyRnyMezJqRFxsA2Sx6YodS8lsiYkZbJZg0LsfU2HJ7f1/sHpZXLtnMlXyx41xYKJVPv6eU679pSwX8dknTkpZe9a6vqqk29ayptJ+L0nH//Utl2H1Q07PJSZNtO5/xHQLE45F1bLha96ngEBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhiwmbBZTNZpdN+mUnp5HHvvpmkLbPLkjUWG/PAIkO+W1Twy1Z6W8lwZJ1/zNhba3G27YzL/sFnJWOOWcmwXyoq/XPJJClR5Z/Z5RK27LCB4z2m+jcOHfKuzdTUmHon0/6ZhErYTpY48t8v5ZItZy5Z9D8Po7yptTIZW1hfbcY/x65/yNb7eNFwbjlbnl7ZkDEYO//jE5f9ankEBAAIwjyAnnvuOV155ZVqbm5WFEV69NFHR33dOac77rhDTU1Nqqys1MqVK/Xiiy+O1XoBAFOEeQANDAxoyZIluv/++0/49XvvvVff/OY39cADD2jXrl2qrq7WqlWrNDw8fNqLBQBMHebngNasWaM1a9ac8GvOOd1333360pe+pKuuukqS9N3vflcNDQ169NFHdd11153eagEAU8aYPgd08OBBdXV1aeXKlSOfy+VyWrZsmXbs2HHC/5PP59XX1zfqBgCY+sZ0AHV1dUmSGhoaRn2+oaFh5Gt/rL29XblcbuTW0tIylksCAExQwV8Ft3HjRvX29o7cDh8+HHpJAIAzYEwHUGNjoySpu7t71Oe7u7tHvvbHstms6urqRt0AAFPfmA6gefPmqbGxUdu2bRv5XF9fn3bt2qXW1tax/FYAgEnO/Cq448ePa//+/SMfHzx4UHv37lV9fb3mzJmj2267Tf/wD/+g8847T/PmzdOXv/xlNTc36+qrrx7LdQMAJjnzANq9e7c+8YlPjHy8YcMGSdK6deu0efNmff7zn9fAwIBuuukm9fT06GMf+5iefPJJVVRUmL5PuRgrIb9YloxnZI8k1defY1pHOl3pXTtc7DH1ThqieCpki3qJDYk2cdkWr1Is2uJy8iX/eJCU5zEf6T3s33vQ2R7wl9Jp79pM1nZ88rY0Fg0ZYp5qKmtNvZNJ/+unr6/f1Dtf8t9QZzw+qWTWvzbhv42SVI5s21lX4X8evvHm66beb7zpX5ur9N8nkqTY//hUVld715bkd59iHkDLly+Xe5fwsCiKdPfdd+vuu++2tgYAnEWCvwoOAHB2YgABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCMEfxnCmFQl6x88spSmX8848G8yXTOoYG8961paGiqfeCOfO9aysqqky9X3zlgHdtyZAHJUmlsm0fFg39h4cHTb2V9M9rO/fchabWFbUz/GuNGVzJpG2fF4v+WWP5Yf9zVpKiyH8fDg8NmXqnNOxdm6y25bXNWOB/TWRStqy+o909pvpy2T+rr7LCtpaGWf77JeVs52Gx4H+f1dtzzLu27JlzySMgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQEzaKp1wuK0r4zcco4b8Zb/T02xYS+0dsJBK2iI3qmlrv2nTSFrGRTvvHdxSN0Tou4b9PJKm2us67dsD5RXi8bdrMRu/ahRdeZOqdyBjij2LbuuPYFttUKPlH8WQy/tE6kpRM+l8/FYbYK0kaGuj1rs0Pv2nqnR/yj20qFW3RR2++abuW+wf9r+Wk9Vou+8cZ1Z4z3dRbht0SG+4LSyW/+xQeAQEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCmLBZcFU11d55Zskq/xymctk/U0uSuo+97l1bVeefeSZJvz10yLs2Ltvy10oZ513rbK1Vjm25Wn3H/fP3ps+YYeo9s3G2d+2Lv33J1FuGjMFsyvazXDpty2urqvbPpRsaGjL1jiL/41lXmzP1rqz0X3cmbTsRB/r8z6uuV7tNvTsPvmZbS8H/2i/HeVPvgiGqsZCsMPVOJv3P25nNc71riwW/+1keAQEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgpiwUTxxHCuOy161kSUaJrLN3K43jnnX1ieTpt6JSkO9s/UuqehdWxtXmnoP5IdN9ccH/esvWrTI1NsZ4nJe6/I/lpLk5B8NU5m1XUrZrC0yZdAQr+NkyG6RlDWsfShvi5Gpz9V719ZVZk2904a7r54e27HvLfjH/EjSsX7/41NXbTv22apq79pXXn/T1DtpeAxy4bwF3rVFz/OER0AAgCAYQACAIMwD6LnnntOVV16p5uZmRVGkRx99dNTXr7/+ekVRNOq2evXqsVovAGCKMA+ggYEBLVmyRPfff/9Ja1avXq2jR4+O3B566KHTWiQAYOoxvwhhzZo1WrNmzbvWZLNZNTY2nvKiAABT37g8B7R9+3bNmjVL559/vm655RYdO3byV6Dk83n19fWNugEApr4xH0CrV6/Wd7/7XW3btk3/9E//pI6ODq1Zs0bl8olfUt3e3q5cLjdya2lpGeslAQAmoDF/H9B111038u+LLrpIixcv1oIFC7R9+3atWLHiHfUbN27Uhg0bRj7u6+tjCAHAWWDcX4Y9f/58zZgxQ/v37z/h17PZrOrq6kbdAABT37gPoFdeeUXHjh1TU1PTeH8rAMAkYv4V3PHjx0c9mjl48KD27t2r+vp61dfX66677tLatWvV2NioAwcO6POf/7zOPfdcrVq1akwXDgCY3MwDaPfu3frEJz4x8vHbz9+sW7dOmzZt0r59+/Sv//qv6unpUXNzs6644gr9/d//vbJZW85TeXhAUTntVTs4POjdN478870kKTbku/UdP27qXRj2z0grlG3rrqzyz5tqbJpp6l1MOlN9Iu13HCVpzux5pt5db/zOu3aot9fUO5P0/wXBcf8oMElSscqWv1cuV3nXpgzrlqRiwf/cSg4NmHoXhv13TFw/w9S7sjrn39vZrp/jg7ZrufvNHu/aVHKaqXdN5J912ddny4Krq6zxLy4bMgZjv1rzAFq+fLmcO/kd0FNPPWVtCQA4C5EFBwAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIYsz/HtBYyZeKKssvc6xkyHcbPskfxjuphH8WnMr+mU2SVCr552olIsM6JFXJP3+tyrZsva/S9iczsjPrvWvnzrFlwS1I+udT1Rhy/SSpKuu/D5WyZY0lMxlTfdqQpxclbD9Xvlu01jtqI9v1Uzb0TiZt+6T3zX7v2pQxi7JkzF5MpPyPTym2XXCFYtG7Npk0noeG83bQkI9XLBS86ngEBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIYsJG8QwWC0o6v8iKYUMEzvHhYdM6hob8o0TmNzeZei86d7Z3bdr4o4IhnUjZyH8bJemCJv91S9Ks8y70rr3oI0tMvc+bWeldawt6GV+2PW6vt7CEtxiDrEy9bSEy0ptF/+v+4KtHTL1/tO2npnpnWHzf4KCpd1+vfwROXGW7S7fE/Pz28MveteWSX0QWj4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQUzYLLiBoWElU37L6xvyz3frGxwyrSOZqPCuHRqyZTz97ne/865NGYOyyoZ8tzhha55KVJnqZ6QWete+dOD/mXp37jrmXTu9ocHUu6HFP/OuHNvS2srGcLeCZ7aWJCUTtp8rU8mkd61L+NdKUmmg37v2wK9+YeqdSvtfm5VJ/1pJqsna6gdKee/awrDtPmiw503v2qpkztS7aEjgGx4qeNfGZMEBACYyBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCICRvFMzwwrIRnREipVPbuWxgumtZRUZX1ru03RvEceOVV71oX2SJQkpbIlMh//0lSotIWU5Lc9TPv2t2PPm7qPWiINXn/h5eaev/P1Vd616Yq6ky9U+mMqf7I0aPetTU1NabetXWGtSfSpt6v/nqvd+2z//t/mXpHg/4RXOc0t5h61xizr/rTsXdtnLTd7dalZ3jXJrK28yqT9j+eyWy1d225VNTLHnU8AgIABGEaQO3t7br44otVW1urWbNm6eqrr1ZnZ+eomuHhYbW1tWn69OmqqanR2rVr1d3dPaaLBgBMfqYB1NHRoba2Nu3cuVNPP/20isWirrjiCg0MDIzU3H777Xr88cf1yCOPqKOjQ0eOHNE111wz5gsHAExupl9GPvnkk6M+3rx5s2bNmqU9e/bosssuU29vr77zne9oy5YtuvzyyyVJDz74oC644ALt3LlTH/3oR8du5QCASe20ngPq7e2VJNXX10uS9uzZo2KxqJUrV47ULFy4UHPmzNGOHTtO2COfz6uvr2/UDQAw9Z3yAIrjWLfddpsuvfRSLVq0SJLU1dWlTCajadOmjaptaGhQV1fXCfu0t7crl8uN3FpabK9WAQBMTqc8gNra2vTCCy/o4YcfPq0FbNy4Ub29vSO3w4cPn1Y/AMDkcErvA1q/fr2eeOIJPffcc5o9+w9/trixsVGFQkE9PT2jHgV1d3ersbHxhL2y2ayyWf/32gAApgbTIyDnnNavX6+tW7fq2Wef1bx580Z9fenSpUqn09q2bdvI5zo7O3Xo0CG1traOzYoBAFOC6RFQW1ubtmzZoscee0y1tbUjz+vkcjlVVlYql8vphhtu0IYNG1RfX6+6ujrdeuutam1t5RVwAIBRTANo06ZNkqTly5eP+vyDDz6o66+/XpL09a9/XYlEQmvXrlU+n9eqVav07W9/e0wWCwCYOiLnnAu9iP+ur69PuVxOTfPPV8Izz6wU+f8msX/QPztMkpKGbKXp5+RMvasq/J/7KsuWBZc21CdlzIKrsj11WD3on79X+8ZxU+/KSv9jPzztHFPvqHGOd20qN8vUO5ux5ekVhvOmeot0xj8PrGzIXZSk4qsHvWvdy53vXfTfZA37ZLDSlo8Xz2k21ffH/jmQhrsrSVLa+f+HsjEzUoZ7f8ugKBWL2vnjJ9Tb26u6d8kaJAsOABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDEKf05hjOhb6BPUcJzPnpG9khSqWxLHnLD/tEj5UKlqXdk+DMU2ZTxZ4Uo8i6NjT+HZI1xHxcs/ZB37azsyWM7TiTR/zvv2uO+59PvdRvOleF0bOo9VLT95d8o6X88E4ZjL0lxwn87o4x/rJIkzZrX5F07fd77TL2rqv2vn5rGBlPvvb+2xQINvewfOZQ2Hp/IEILjYtt5WNb43E/Ezq8vj4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQUzYLLg4LntnIEWGbKVMOmNaR3VllXdtjaFWklJJ/92fMuaYWRjjo5SQLQtu0f9Y5l37Pz/2CVPv4SjvX2zch6mE5fKw5XsVEv4Zg5IUG9onDdmIkkxLLxgy6SQpKvtvZ0XRdiKmK/2v5Yxt2Xr1m9821b/80m/915L2z7CTJBeXvGvLZds+dJ6ZbZIUW2I0PWt5BAQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACGLCRvFUZLKKPKNTMtlK777ZqmrTOqoM8ToVGVvMTyrlv/tjZ8nBkJKG8pQxRiYu2dby25df9a6dObvL1Dtd53880xlbRE0m7R+Bkk7bfpYrFwum+lLJfy1WyaT/2qOEMUam4B8NUxg2xCpJKpaL3rUDAz2m3n19fab6hCEOzKpc8o8zMt5NKBH5H/uEobdvVx4BAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIKYuFlw2awSCb/srooq/7w2S26cJGVTae/apDEPylKf8twXb4ti/+CmyBggNWzI4JKk117v9q7tevWIqXfNEUO2ny2qT6WMIcfM2bLdertfNy7GPw+sXPavlaSsIcPwnIZmU++y/y5U5GzrdpHh+Az1mnpbs+Biw/UWx4adIimd9r8PSthaq1Dy/w+WRytkwQEAJjTTAGpvb9fFF1+s2tpazZo1S1dffbU6OztH1SxfvlxRFI263XzzzWO6aADA5GcaQB0dHWpra9POnTv19NNPq1gs6oorrtDAwMCouhtvvFFHjx4dud17771jumgAwORneg7oySefHPXx5s2bNWvWLO3Zs0eXXXbZyOerqqrU2Ng4NisEAExJp/UcUG/vW0/s1dfXj/r89773Pc2YMUOLFi3Sxo0bNTg4eNIe+XxefX19o24AgKnvlF8FF8exbrvtNl166aVatGjRyOc//elPa+7cuWpubta+ffv0hS98QZ2dnfrhD394wj7t7e266667TnUZAIBJ6pQHUFtbm1544QX99Kc/HfX5m266aeTfF110kZqamrRixQodOHBACxYseEefjRs3asOGDSMf9/X1qaWl5VSXBQCYJE5pAK1fv15PPPGEnnvuOc2ePftda5ctWyZJ2r9//wkHUDabVTZr+zvzAIDJzzSAnHO69dZbtXXrVm3fvl3z5s17z/+zd+9eSVJTU9MpLRAAMDWZBlBbW5u2bNmixx57TLW1terq6pIk5XI5VVZW6sCBA9qyZYv+7M/+TNOnT9e+fft0++2367LLLtPixYvHZQMAAJOTaQBt2rRJ0ltvNv3vHnzwQV1//fXKZDJ65plndN9992lgYEAtLS1au3atvvSlL43ZggEAU4P5V3DvpqWlRR0dHae1oLdVZrJKJP3yzyor/fPdkilbIFjKcw3SqWTB+b8KPkrYXjHvZMh3sy1bccGWe/bqK/u9a2fNnGXqXVcxzbs2k7Ud+2yFf30ybcvqq6k2ZNhJqjLUl0olU+/+/n7v2t7f2TLshvPD3rVD+ZO/XeNESiX/83DouC0L7vjx46b6jCFPz3RtSrJcoLm6OlPn3v6B9y76vVLZkC/pmY1HFhwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIIhT/ntA4y2VyijpGYOTzfj/OYdkZItMSRjidaxxOaYEnDg29ZZh3UVn621I5JAkvfTyb71rDx46bOqdyVR51+ay/rWSVFPhH/FUVVtj6p2tn2aqr6jxj+IpG6N43v7Lxj5Kv7NF2gwaevcM2P4acr6Q966tq7Mdn3NytkibdMoQq2W8Dyobrv3BIf/oI0lyhnshy7Kj2K8vj4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQUzYLLhsJqtk0m95Kc/MOElKGIPMEgn/rCRnHOfOksHmbOuODeu2ZE1JUmz8ucXJ//gcO3bM1Fvl1/17O1P6nmLDfikaj32ctK3FwhnPFUt9ulw29U7G/r3LhnNWkhJp/7uvypqMrXdkvN4M50rJmNWX8rwflCRXtl3LxbL/Wjzj3SRJpdjvPOEREAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgiAkbxRNFkaLIL/vBEiWSNMT2vL0O/2JjRI0lecQcr+IfyWENhbH+1JJJp71ra6qrTb0LxaJ/cckWUyLDPkzIdnzMQTzG429iiJ1JGc/xlCFepzLjf55IUqXhXKnIZE29LdE6kpTJ+Ef9WGolyRnijBLG+zcZ7t9KhuvBee4/HgEBAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgpiwWXCJKKFEwm8+plL+m5FOGDfZkMMUJY1ZcIZ4rzgum3pbfrZIGYPJXNL2Hyz7PJO25WTFhp1YUsnU2xLWlzScJ2/1Ntabw+PGR9KYeZdK+Z+H1dU1pt6W3EBLHqFkz4IrlfzPLVO+pKSKbIV3bSpl286hfN671hmuB7LgAAATmmkAbdq0SYsXL1ZdXZ3q6urU2tqqH/3oRyNfHx4eVltbm6ZPn66amhqtXbtW3d3dY75oAMDkZxpAs2fP1j333KM9e/Zo9+7duvzyy3XVVVfpl7/8pSTp9ttv1+OPP65HHnlEHR0dOnLkiK655ppxWTgAYHIzPSFy5ZVXjvr4H//xH7Vp0ybt3LlTs2fP1ne+8x1t2bJFl19+uSTpwQcf1AUXXKCdO3fqox/96NitGgAw6Z3yc0DlclkPP/ywBgYG1Nraqj179qhYLGrlypUjNQsXLtScOXO0Y8eOk/bJ5/Pq6+sbdQMATH3mAfSLX/xCNTU1ymazuvnmm7V161ZdeOGF6urqUiaT0bRp00bVNzQ0qKur66T92tvblcvlRm4tLS3mjQAATD7mAXT++edr79692rVrl2655RatW7dOv/rVr055ARs3blRvb+/I7fDhw6fcCwAweZjfB5TJZHTuuedKkpYuXar/+q//0je+8Q1de+21KhQK6unpGfUoqLu7W42NjSftl81mlc3a/l47AGDyO+33AcVxrHw+r6VLlyqdTmvbtm0jX+vs7NShQ4fU2tp6ut8GADDFmB4Bbdy4UWvWrNGcOXPU39+vLVu2aPv27XrqqaeUy+V0ww03aMOGDaqvr1ddXZ1uvfVWtba28go4AMA7mAbQa6+9pr/4i7/Q0aNHlcvltHjxYj311FP60z/9U0nS17/+dSUSCa1du1b5fF6rVq3St7/97VNaWCLhH8UTG2JQYtkiNjJJ/13kjHEslhiZhCEGQ5KcoXfSGA3ioqSpPmuIQYmMCTVZQ/TIULlo6h2X/eOPEsZjH1mzdRL+9XHZdo5bluJ5SY7IZPyvn+qKKltvw7WZjGwLTyZt57glXsca85MwRHxFxu1Mpfy308WWKB6/aydylnuqM6Cvr0+5XE4fvuTjSnpmvKWy/vlh2YTtxDINIOOutAwga++y8z/JS8YBVDLevw0X/XOyCgXbkCgYMrgYQCddjP8yJtAAyhquzazhPkKSMmnb0+O+PyxbayWppsY/Iy+ZsGXBFUoF79qiYXCWigU98+j31dvbq7q6upPWkQUHAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIwpyGPd7efsd/uez/DncZoiqSCWMMxjimFZwtSQjlkn+igOm4S4oN9Zbat+oNGzreSQiGKKbxTEKQMTfFM5FFklQ2pFpIUtmwllLStr8Txg0dzySEYtE/rSBO2NZdNCQhlExJCG+ljrzX/daEG0D9/f2SpP+75+R/RRUAMPH19/crl8ud9OsTLgsujmMdOXJEtbW1owL++vr61NLSosOHD79rttBkx3ZOHWfDNkps51QzFtvpnFN/f7+am5vf9RHfhHsElEgkNHv27JN+va6ubkof/LexnVPH2bCNEts51Zzudr7bI5+38SIEAEAQDCAAQBCTZgBls1ndeeedymazoZcyrtjOqeNs2EaJ7ZxqzuR2TrgXIQAAzg6T5hEQAGBqYQABAIJgAAEAgmAAAQCCmDQD6P7779f73/9+VVRUaNmyZfrP//zP0EsaU1/5ylcURdGo28KFC0Mv67Q899xzuvLKK9Xc3KwoivToo4+O+rpzTnfccYeamppUWVmplStX6sUXXwyz2NPwXtt5/fXXv+PYrl69OsxiT1F7e7suvvhi1dbWatasWbr66qvV2dk5qmZ4eFhtbW2aPn26ampqtHbtWnV3dwda8anx2c7ly5e/43jefPPNgVZ8ajZt2qTFixePvNm0tbVVP/rRj0a+fqaO5aQYQN///ve1YcMG3Xnnnfr5z3+uJUuWaNWqVXrttddCL21MffCDH9TRo0dHbj/96U9DL+m0DAwMaMmSJbr//vtP+PV7771X3/zmN/XAAw9o165dqq6u1qpVqzQ8PHyGV3p63ms7JWn16tWjju1DDz10Bld4+jo6OtTW1qadO3fq6aefVrFY1BVXXKGBgYGRmttvv12PP/64HnnkEXV0dOjIkSO65pprAq7azmc7JenGG28cdTzvvffeQCs+NbNnz9Y999yjPXv2aPfu3br88st11VVX6Ze//KWkM3gs3SRwySWXuLa2tpGPy+Wya25udu3t7QFXNbbuvPNOt2TJktDLGDeS3NatW0c+juPYNTY2uq9+9asjn+vp6XHZbNY99NBDAVY4Nv54O51zbt26de6qq64Ksp7x8tprrzlJrqOjwzn31rFLp9PukUceGan59a9/7SS5HTt2hFrmafvj7XTOuT/5kz9xf/3Xfx1uUePknHPOcf/8z/98Ro/lhH8EVCgUtGfPHq1cuXLkc4lEQitXrtSOHVMrMfvFF19Uc3Oz5s+fr8985jM6dOhQ6CWNm4MHD6qrq2vUcc3lclq2bNmUO66StH37ds2aNUvnn3++brnlFh07diz0kk5Lb2+vJKm+vl6StGfPHhWLxVHHc+HChZozZ86kPp5/vJ1v+973vqcZM2Zo0aJF2rhxowYHB0Msb0yUy2U9/PDDGhgYUGtr6xk9lhMujPSPvfHGGyqXy2poaBj1+YaGBv3mN78JtKqxt2zZMm3evFnnn3++jh49qrvuuksf//jH9cILL6i2tjb08sZcV1eXJJ3wuL79tali9erVuuaaazRv3jwdOHBAf/d3f6c1a9Zox44dSiaToZdnFsexbrvtNl166aVatGiRpLeOZyaT0bRp00bVTubjeaLtlKRPf/rTmjt3rpqbm7Vv3z594QtfUGdnp374wx8GXK3dL37xC7W2tmp4eFg1NTXaunWrLrzwQu3du/eMHcsJP4DOFmvWrBn59+LFi7Vs2TLNnTtXP/jBD3TDDTcEXBlO13XXXTfy74suukiLFy/WggULtH37dq1YsSLgyk5NW1ubXnjhhUn/HOV7Odl23nTTTSP/vuiii9TU1KQVK1bowIEDWrBgwZle5ik7//zztXfvXvX29urf/u3ftG7dOnV0dJzRNUz4X8HNmDFDyWTyHa/A6O7uVmNjY6BVjb9p06bpAx/4gPbv3x96KePi7WN3th1XSZo/f75mzJgxKY/t+vXr9cQTT+gnP/nJqD+b0tjYqEKhoJ6enlH1k/V4nmw7T2TZsmWSNOmOZyaT0bnnnqulS5eqvb1dS5Ys0Te+8Y0zeiwn/ADKZDJaunSptm3bNvK5OI61bds2tba2BlzZ+Dp+/LgOHDigpqam0EsZF/PmzVNjY+Oo49rX16ddu3ZN6eMqSa+88oqOHTs2qY6tc07r16/X1q1b9eyzz2revHmjvr506VKl0+lRx7Ozs1OHDh2aVMfzvbbzRPbu3StJk+p4nkgcx8rn82f2WI7pSxrGycMPP+yy2azbvHmz+9WvfuVuuukmN23aNNfV1RV6aWPmb/7mb9z27dvdwYMH3X/8x3+4lStXuhkzZrjXXnst9NJOWX9/v3v++efd888/7yS5r33ta+755593L7/8snPOuXvuucdNmzbNPfbYY27fvn3uqquucvPmzXNDQ0OBV27zbtvZ39/vPve5z7kdO3a4gwcPumeeecZ9+MMfduedd54bHh4OvXRvt9xyi8vlcm779u3u6NGjI7fBwcGRmptvvtnNmTPHPfvss2737t2utbXVtba2Bly13Xtt5/79+93dd9/tdu/e7Q4ePOgee+wxN3/+fHfZZZcFXrnNF7/4RdfR0eEOHjzo9u3b5774xS+6KIrcj3/8Y+fcmTuWk2IAOefct771LTdnzhyXyWTcJZdc4nbu3Bl6SWPq2muvdU1NTS6Tybj3ve997tprr3X79+8PvazT8pOf/MRJesdt3bp1zrm3Xor95S9/2TU0NLhsNutWrFjhOjs7wy76FLzbdg4ODrorrrjCzZw506XTaTd37lx34403Trofnk60fZLcgw8+OFIzNDTk/uqv/sqdc845rqqqyn3yk590R48eDbfoU/Be23no0CF32WWXufr6epfNZt25557r/vZv/9b19vaGXbjRX/7lX7q5c+e6TCbjZs6c6VasWDEyfJw7c8eSP8cAAAhiwj8HBACYmhhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCD+PxxNlAwp/sz7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show some samples in the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "imgplot = plt.imshow(X_train[5])\n",
        "plt.show()\n",
        "imgplot = plt.imshow(X_test[10])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQfcepEOgX3t"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "# Preprocess data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_val = X_val.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, Conv2D, Dense, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "mtiAqK6kRJmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "The model is customized to work well on Cirfar10\n",
        "The model is based on MobileNet-v2. The model in this project includes some architectures and other modifications,.... using Tensorflow and functional API as well as some regularization and different kernel sizes..."
      ],
      "metadata": {
        "id": "jbj63V6DRXv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input layer\n",
        "input_layer = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_layer\")\n",
        "\n",
        "# Convolutional Layer 1\n",
        "x1 = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", name=\"x1\", kernel_initializer='he_uniform')(input_layer)\n",
        "x2 = tf.keras.layers.BatchNormalization(name=\"x2\")(x1)\n",
        "x3 = tf.keras.layers.ReLU(name=\"x3\")(x2)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x4 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x4\")(x3)\n",
        "x5 = tf.keras.layers.BatchNormalization(name=\"x5\")(x4)\n",
        "x6 = tf.keras.layers.ReLU(name=\"x6\")(x5)\n",
        "\n",
        "# Convolutional Projection Layer\n",
        "x7 = tf.keras.layers.Conv2D(32, (1, 1), padding=\"same\", name=\"x7\", kernel_initializer='he_uniform')(x6)\n",
        "x8 = tf.keras.layers.BatchNormalization(name=\"x8\")(x7)\n",
        "x9 = Dropout(0.15)(x8)\n",
        "\n",
        "# Block 1 - Expand Layer\n",
        "x10 = tf.keras.layers.Conv2D(96, (3, 3), padding=\"same\", name=\"x10\")(x9)#(96, (1, 1)\n",
        "x11 = tf.keras.layers.BatchNormalization(name=\"x11\")(x10)\n",
        "x12 = tf.keras.layers.ReLU(name=\"x13\")(x11)\n",
        "x13 = Dropout(0.15)(x12)\n",
        "\n",
        "# Zero Padding Layer\n",
        "x14 = tf.keras.layers.ZeroPadding2D(padding=((0, 1), (0, 1)), name=\"x14\")(x13)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x15 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"valid\", strides=(2, 2), name=\"x15\")(x14)\n",
        "x15 = tf.keras.layers.BatchNormalization(name=\"x15_batch\")(x15)\n",
        "x16 = tf.keras.layers.ReLU(name=\"x17\")(x15)\n",
        "x17 = Dropout(0.15)(x16)\n",
        "# Project Layer\n",
        "x18 = tf.keras.layers.Conv2D(24, (1, 1), padding=\"same\", name=\"x18\")(x17)\n",
        "x19 = tf.keras.layers.BatchNormalization(name=\"x19\")(x18)\n",
        "\n",
        "#BLOCK 2\n",
        "# Block 2 - Expand Layer\n",
        "x20 = tf.keras.layers.Conv2D(150, (1, 1), padding=\"same\", name=\"x20\", kernel_initializer='he_uniform')(x19)\n",
        "x21 = tf.keras.layers.BatchNormalization(name=\"x21\")(x20)\n",
        "x22 = tf.keras.layers.ReLU(name=\"x22\")(x21)\n",
        "x22 = Dropout(0.15)(x22)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x23 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x23\")(x22)\n",
        "x24 = tf.keras.layers.BatchNormalization(name=\"x24\")(x23)\n",
        "x25 = tf.keras.layers.ReLU(name=\"x26\")(x24)\n",
        "x26 = Dropout(0.15)(x25)\n",
        "\n",
        "# Project Layer\n",
        "x27 = tf.keras.layers.Conv2D(24, (1, 1), padding=\"same\", name=\"x27\")(x26)\n",
        "x28 = tf.keras.layers.BatchNormalization(name=\"x28\")(x27)\n",
        "\n",
        "# Add Layer\n",
        "x29 = tf.keras.layers.Add(name=\"x29\")([x19, x28])\n",
        "\n",
        "# Block 3 - Expand Layer\n",
        "x30 = tf.keras.layers.Conv2D(150, (1, 1), padding=\"same\", name=\"x30\", kernel_initializer='he_uniform')(x29)\n",
        "x31 = tf.keras.layers.BatchNormalization(name=\"x31\")(x30)\n",
        "x32 = tf.keras.layers.ReLU(name=\"x33\")(x31)\n",
        "x33 = Dropout(0.15)(x32)\n",
        "# Zero Padding Layer\n",
        "x34 = tf.keras.layers.ZeroPadding2D(padding=(1, 1), name=\"x34\")(x33)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x35 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"valid\", strides=(2, 2), name=\"x35\")(x34)\n",
        "x36 = tf.keras.layers.BatchNormalization(name=\"x36\")(x35)\n",
        "x37 = tf.keras.layers.ReLU(name=\"x38\")(x36)\n",
        "x38 = Dropout(0.15)(x37)\n",
        "# Project Layer\n",
        "x39 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x39\")(x38)#Conv2D(32, (1, 1)\n",
        "x40 = tf.keras.layers.BatchNormalization(name=\"x40\")(x39)\n",
        "\n",
        "# Block 4 - Expand Layer\n",
        "x41 = tf.keras.layers.Conv2D(200, (1, 1), padding=\"same\", name=\"x41\", kernel_initializer='he_uniform')(x40)\n",
        "x42 = tf.keras.layers.BatchNormalization(name=\"x42\")(x41)\n",
        "x43 = tf.keras.layers.ReLU(name=\"x44\")(x42)\n",
        "x44 = Dropout(0.15)(x43)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x45 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x45\")(x44)\n",
        "x46 = tf.keras.layers.BatchNormalization(name=\"x46\")(x45)\n",
        "x47 = tf.keras.layers.ReLU(name=\"x48\")(x46)\n",
        "x48 = Dropout(0.15)(x47)\n",
        "\n",
        "# Project Layer\n",
        "x49 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x49\", kernel_initializer='he_uniform')(x48)#\n",
        "x50 = tf.keras.layers.BatchNormalization(name=\"x50\")(x49)\n",
        "\n",
        "# Add Layer\n",
        "x51 = tf.keras.layers.Add(name=\"x51\")([x40, x50])\n",
        "\n",
        "# Block 5 - Expand Layer\n",
        "x52 = tf.keras.layers.Conv2D(200, (1, 1), padding=\"same\", name=\"x52\", kernel_initializer='he_uniform')(x51)\n",
        "x53 = tf.keras.layers.BatchNormalization(name=\"x53\")(x52)\n",
        "x54 = tf.keras.layers.ReLU(name=\"x55\")(x53)\n",
        "x55 = Dropout(0.15)(x54)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x56 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x56\")(x55)\n",
        "x57 = tf.keras.layers.BatchNormalization(name=\"x57\")(x56)\n",
        "x58 = tf.keras.layers.ReLU(name=\"x59\")(x57)\n",
        "x59 = Dropout(0.15)(x58)\n",
        "\n",
        "# Project Layer\n",
        "x60 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x60\", kernel_initializer='he_uniform')(x59)\n",
        "x61 = tf.keras.layers.BatchNormalization(name=\"x61\")(x60)\n",
        "\n",
        "# Add Layer\n",
        "x62 = tf.keras.layers.Add(name=\"x62\")([x51, x61])\n",
        "\n",
        "# Block 6 - Expand Layer\n",
        "x63 = tf.keras.layers.Conv2D(200, (1, 1), padding=\"same\", name=\"x63\", kernel_initializer='he_uniform')(x62)\n",
        "x64 = tf.keras.layers.BatchNormalization(name=\"x64\")(x63)\n",
        "x65 = tf.keras.layers.ReLU(name=\"x66\")(x64)\n",
        "x66 = Dropout(0.15)(x65)\n",
        "# Zero Padding Layer\n",
        "x67 = tf.keras.layers.ZeroPadding2D(padding=(1, 1), name=\"x67\")(x66)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x68 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"valid\", strides=(2, 2), name=\"x68\")(x67)\n",
        "x69 = tf.keras.layers.BatchNormalization(name=\"x69\")(x68)\n",
        "x70 = tf.keras.layers.ReLU(name=\"x71\")(x69)\n",
        "x71 = Dropout(0.15)(x70)\n",
        "\n",
        "# Project Layer\n",
        "x72 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x72\")(x71)\n",
        "x73 = tf.keras.layers.BatchNormalization(name=\"x73\")(x72)\n",
        "\n",
        "# Block 7 - Expand Layer\n",
        "x74 = tf.keras.layers.Conv2D(400, (1, 1), padding=\"same\", name=\"x74\", kernel_initializer='he_uniform')(x73)\n",
        "x75 = tf.keras.layers.BatchNormalization(name=\"x75\")(x74)\n",
        "x76 = tf.keras.layers.ReLU(name=\"x77\")(x75)\n",
        "x77 = Dropout(0.15)(x76)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x78 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x78\")(x77)\n",
        "x79 = tf.keras.layers.BatchNormalization(name=\"x79\")(x78)\n",
        "x80 = tf.keras.layers.ReLU(name=\"x81\")(x79)\n",
        "x81 = Dropout(0.15)(x80)\n",
        "# Project Layer\n",
        "x82 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x82\")(x81)\n",
        "x83 = tf.keras.layers.BatchNormalization(name=\"x83\")(x82)\n",
        "\n",
        "# Add Layer\n",
        "x84 = tf.keras.layers.Add(name=\"x84\")([x73, x83])\n",
        "\n",
        "# Block 8 - Expand Layer\n",
        "x85 = tf.keras.layers.Conv2D(400, (1, 1), padding=\"same\", name=\"x85\", kernel_initializer='he_uniform')(x84)\n",
        "x86 = tf.keras.layers.BatchNormalization(name=\"x86\")(x85)\n",
        "x87 = tf.keras.layers.ReLU(name=\"x88\")(x86)\n",
        "x88 = Dropout(0.15)(x87)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x89 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x89\")(x88)\n",
        "x90 = tf.keras.layers.BatchNormalization(name=\"x90\")(x89)\n",
        "x91 = tf.keras.layers.ReLU(name=\"x92\")(x90)\n",
        "x92 = Dropout(0.15)(x91)\n",
        "\n",
        "# Project Layer\n",
        "x93 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x93\", kernel_initializer='he_uniform')(x92)\n",
        "x94 = tf.keras.layers.BatchNormalization(name=\"x94\")(x93)\n",
        "\n",
        "# Add Layer\n",
        "x95 = tf.keras.layers.Add(name=\"x95\")([x84, x94])\n",
        "\n",
        "# Block 9 - Expand Layer\n",
        "x96 = tf.keras.layers.Conv2D(384, (1, 1), padding=\"same\", name=\"x96\", kernel_initializer='he_uniform')(x95)\n",
        "x97 = tf.keras.layers.BatchNormalization(name=\"x97\")(x96)\n",
        "x98 = tf.keras.layers.ReLU(name=\"x99\")(x97)\n",
        "x99 = Dropout(0.15)(x98)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x100 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x100\")(x99)\n",
        "x101 = tf.keras.layers.BatchNormalization(name=\"x101\")(x100)\n",
        "x102 = tf.keras.layers.ReLU(name=\"x103\")(x101)\n",
        "x103 = Dropout(0.15)(x102)\n",
        "\n",
        "# Project Layer\n",
        "x104 = tf.keras.layers.Conv2D(64, (1, 1), padding=\"same\", name=\"x104\", kernel_initializer='he_uniform')(x103)\n",
        "x105 = tf.keras.layers.BatchNormalization(name=\"x105\")(x104)\n",
        "\n",
        "# Add Layer\n",
        "x106 = tf.keras.layers.Add(name=\"x106\")([x95, x105])\n",
        "\n",
        "# Block 10 - Expand Layer\n",
        "x107 = tf.keras.layers.Conv2D(384, (1, 1), padding=\"same\", name=\"x107\", kernel_initializer='he_uniform')(x106)\n",
        "x108 = tf.keras.layers.BatchNormalization(name=\"x108\")(x107)\n",
        "x109 = tf.keras.layers.ReLU(name=\"x110\")(x108)\n",
        "x110 = Dropout(0.15)(x109)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x111 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x111\")(x110)\n",
        "x112 = tf.keras.layers.BatchNormalization(name=\"x112\")(x111)\n",
        "x113 = tf.keras.layers.ReLU(name=\"x114\")(x112)\n",
        "x114 = Dropout(0.2)(x113)\n",
        "\n",
        "# Project Layer\n",
        "x115 = tf.keras.layers.Conv2D(96, (1, 1), padding=\"same\", name=\"x115\", kernel_initializer='he_uniform')(x114)\n",
        "x116 = tf.keras.layers.BatchNormalization(name=\"x116\")(x115)\n",
        "\n",
        "# Block 11 - Expand Layer\n",
        "x117 = tf.keras.layers.Conv2D(576, (1, 1), padding=\"same\", name=\"x117\", kernel_initializer='he_uniform')(x116)\n",
        "x118 = tf.keras.layers.BatchNormalization(name=\"x118\")(x117)\n",
        "x119 = tf.keras.layers.ReLU(name=\"x120\")(x118)\n",
        "x120 = Dropout(0.15)(x119)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x121 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x121\")(x120)\n",
        "x122 = tf.keras.layers.BatchNormalization(name=\"x122\")(x121)\n",
        "x123 = tf.keras.layers.ReLU(name=\"x124\")(x122)\n",
        "x124 = Dropout(0.15)(x123)\n",
        "\n",
        "# Project Layer\n",
        "x125 = tf.keras.layers.Conv2D(96, (1, 1), padding=\"same\", name=\"x125\", kernel_initializer='he_uniform')(x124)\n",
        "x126 = tf.keras.layers.BatchNormalization(name=\"x126\")(x125)\n",
        "\n",
        "# Add Layer\n",
        "x127 = tf.keras.layers.Add(name=\"x127\")([x116, x126])\n",
        "\n",
        "# Block 12 - Expand Layer\n",
        "x128 = tf.keras.layers.Conv2D(576, (1, 1), padding=\"same\", name=\"x128\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x127)\n",
        "x129 = tf.keras.layers.BatchNormalization(name=\"x129\")(x128)\n",
        "x130 = tf.keras.layers.ReLU(name=\"x131\")(x129)\n",
        "x131 = Dropout(0.15)(x130)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x132 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x132\")(x131)\n",
        "x133 = tf.keras.layers.BatchNormalization(name=\"x133\")(x132)\n",
        "x134 = tf.keras.layers.ReLU(name=\"x135\")(x133)\n",
        "x135 = Dropout(0.15)(x134)\n",
        "\n",
        "# Project Layer\n",
        "x136 = tf.keras.layers.Conv2D(96, (1, 1), padding=\"same\", name=\"x136\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x135)\n",
        "x137 = tf.keras.layers.BatchNormalization(name=\"x137\")(x136)\n",
        "\n",
        "# Add Layer\n",
        "x138 = tf.keras.layers.Add(name=\"x138\")([x127, x137])\n",
        "\n",
        "###################################################################\n",
        "# Block 12' - Expand Layer\n",
        "x128 = tf.keras.layers.Conv2D(576, (1, 1), padding=\"same\", name=\"x128_\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x138)\n",
        "x129 = tf.keras.layers.BatchNormalization(name=\"x129_\")(x128)\n",
        "x130 = tf.keras.layers.ReLU(name=\"x131_\")(x129)\n",
        "x131 = Dropout(0.15)(x130)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x132 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x132_\")(x131)\n",
        "x133 = tf.keras.layers.BatchNormalization(name=\"x133_\")(x132)\n",
        "x134 = tf.keras.layers.ReLU(name=\"x135_\")(x133)\n",
        "x135 = Dropout(0.15)(x134)\n",
        "\n",
        "# Project Layer\n",
        "x136 = tf.keras.layers.Conv2D(96, (1, 1), padding=\"same\", name=\"x136_\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x135)\n",
        "x137 = tf.keras.layers.BatchNormalization(name=\"x137_\")(x136)\n",
        "\n",
        "# Add Layer\n",
        "x138 = tf.keras.layers.Add(name=\"x138_\")([x127, x137])\n",
        "############################################################################\n",
        "\n",
        "# Block 13 - Expand Layer\n",
        "x139 = tf.keras.layers.Conv2D(576, (1, 1), padding=\"same\", name=\"x139\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x138)\n",
        "x140 = tf.keras.layers.BatchNormalization(name=\"x140\")(x139)\n",
        "x141 = tf.keras.layers.ReLU(name=\"x142\")(x140)\n",
        "x142 = Dropout(0.15)(x141)\n",
        "\n",
        "# Zero Padding Layer\n",
        "x143 = tf.keras.layers.ZeroPadding2D(name=\"x143\")(x142)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x144 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"valid\", name=\"x144\")(x143)\n",
        "x145 = tf.keras.layers.BatchNormalization(name=\"x145\")(x144)\n",
        "x146 = tf.keras.layers.ReLU(name=\"x147\")(x145)\n",
        "x147 = Dropout(0.15)(x146)\n",
        "\n",
        "# Project Layer\n",
        "x148 = tf.keras.layers.Conv2D(160, (1, 1), padding=\"same\", name=\"x148\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x147)\n",
        "x149 = tf.keras.layers.BatchNormalization(name=\"x149\")(x148)\n",
        "\n",
        "# Block 14 - Expand Layer\n",
        "x150 = tf.keras.layers.Conv2D(960, (1, 1), padding=\"same\", name=\"x150\")(x149)\n",
        "x151 = tf.keras.layers.BatchNormalization(name=\"x151\")(x150)\n",
        "x152 = tf.keras.layers.ReLU(name=\"x153\")(x151)\n",
        "x153 = Dropout(0.15)(x152)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x154 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x154\")(x153)\n",
        "x155 = tf.keras.layers.BatchNormalization(name=\"x155\")(x154)\n",
        "x156 = tf.keras.layers.ReLU(name=\"x157\")(x155)\n",
        "x157 = Dropout(0.15)(x156)\n",
        "\n",
        "# Project Layer\n",
        "x158 = tf.keras.layers.Conv2D(160, (1, 1), padding=\"same\", name=\"x158\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x157)\n",
        "x159 = tf.keras.layers.BatchNormalization(name=\"x159\")(x158)\n",
        "\n",
        "# Add Layer\n",
        "x160 = tf.keras.layers.Add(name=\"x160\")([x149, x159])\n",
        "\n",
        "# Block 15 - Expand Layer\n",
        "x161 = tf.keras.layers.Conv2D(960, (1, 1), padding=\"same\", name=\"x161\")(x160)\n",
        "x162 = tf.keras.layers.BatchNormalization(name=\"x162\")(x161)\n",
        "x163 = tf.keras.layers.ReLU(name=\"x164\")(x162)\n",
        "x164 = Dropout(0.15)(x163)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x165 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x165\")(x164)\n",
        "x166 = tf.keras.layers.BatchNormalization(name=\"x166\")(x165)\n",
        "x167 = tf.keras.layers.ReLU(name=\"x168\")(x166)\n",
        "x168 = Dropout(0.15)(x167)\n",
        "\n",
        "# Project Layer\n",
        "x169 = tf.keras.layers.Conv2D(160, (1, 1), padding=\"same\", name=\"x169\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x168)\n",
        "x170 = tf.keras.layers.BatchNormalization(name=\"x170\")(x169)\n",
        "\n",
        "# Add Layer\n",
        "x171 = tf.keras.layers.Add(name=\"x171\")([x160, x170])\n",
        "\n",
        "# Block 16 - Expand Layer\n",
        "x172 = tf.keras.layers.Conv2D(960, (1, 1), padding=\"same\", name=\"x172\")(x171)\n",
        "x173 = tf.keras.layers.BatchNormalization(name=\"x173\")(x172)\n",
        "x174 = tf.keras.layers.ReLU(name=\"x175\")(x173)\n",
        "x175 = Dropout(0.15)(x174)\n",
        "\n",
        "# Depthwise Convolution Layer\n",
        "x176 = tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\", name=\"x176\")(x175)\n",
        "x177 = tf.keras.layers.BatchNormalization(name=\"x177\")(x176)\n",
        "x178 = tf.keras.layers.ReLU(name=\"x179\")(x177)\n",
        "x179 = Dropout(0.15)(x178)\n",
        "\n",
        "# Project Layer\n",
        "x180 = tf.keras.layers.Conv2D(320, (1, 1), padding=\"same\", name=\"x180\", kernel_regularizer=regularizers.l2(0.01), kernel_initializer='he_uniform')(x179)\n",
        "x181 = tf.keras.layers.BatchNormalization(name=\"x181\")(x180)\n",
        "\n",
        "# Convolution Layer\n",
        "x182 = tf.keras.layers.Conv2D(1280, (1, 1), padding=\"same\", name=\"x182\")(x181)\n",
        "x183 = tf.keras.layers.BatchNormalization(name=\"x183\")(x182)\n",
        "out_put = tf.keras.layers.ReLU(name=\"out_put\")(x183)\n",
        "\n",
        "# Final layer\n",
        "x_final = GlobalAveragePooling2D()(out_put)\n",
        "x_final = Dense(128, activation='relu')(x_final)\n",
        "x_final = Dense(10, activation='softmax')(x_final)\n",
        "model = tf.keras.Model(input_layer, x_final, name=\"Model1\")\n",
        "\n"
      ],
      "metadata": {
        "id": "915kUKPYRXTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgGmv5Ffr2jo",
        "outputId": "4df36ad6-c956-41f0-e37f-80c85ca6477b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_layer (InputLayer)    [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " x1 (Conv2D)                 (None, 32, 32, 64)           1792      ['input_layer[0][0]']         \n",
            "                                                                                                  \n",
            " x2 (BatchNormalization)     (None, 32, 32, 64)           256       ['x1[0][0]']                  \n",
            "                                                                                                  \n",
            " x3 (ReLU)                   (None, 32, 32, 64)           0         ['x2[0][0]']                  \n",
            "                                                                                                  \n",
            " x4 (DepthwiseConv2D)        (None, 32, 32, 64)           640       ['x3[0][0]']                  \n",
            "                                                                                                  \n",
            " x5 (BatchNormalization)     (None, 32, 32, 64)           256       ['x4[0][0]']                  \n",
            "                                                                                                  \n",
            " x6 (ReLU)                   (None, 32, 32, 64)           0         ['x5[0][0]']                  \n",
            "                                                                                                  \n",
            " x7 (Conv2D)                 (None, 32, 32, 32)           2080      ['x6[0][0]']                  \n",
            "                                                                                                  \n",
            " x8 (BatchNormalization)     (None, 32, 32, 32)           128       ['x7[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 32, 32, 32)           0         ['x8[0][0]']                  \n",
            "                                                                                                  \n",
            " x10 (Conv2D)                (None, 32, 32, 96)           27744     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " x11 (BatchNormalization)    (None, 32, 32, 96)           384       ['x10[0][0]']                 \n",
            "                                                                                                  \n",
            " x13 (ReLU)                  (None, 32, 32, 96)           0         ['x11[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 32, 32, 96)           0         ['x13[0][0]']                 \n",
            "                                                                                                  \n",
            " x14 (ZeroPadding2D)         (None, 33, 33, 96)           0         ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " x15 (DepthwiseConv2D)       (None, 16, 16, 96)           960       ['x14[0][0]']                 \n",
            "                                                                                                  \n",
            " x15_batch (BatchNormalizat  (None, 16, 16, 96)           384       ['x15[0][0]']                 \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " x17 (ReLU)                  (None, 16, 16, 96)           0         ['x15_batch[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 16, 16, 96)           0         ['x17[0][0]']                 \n",
            "                                                                                                  \n",
            " x18 (Conv2D)                (None, 16, 16, 24)           2328      ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " x19 (BatchNormalization)    (None, 16, 16, 24)           96        ['x18[0][0]']                 \n",
            "                                                                                                  \n",
            " x20 (Conv2D)                (None, 16, 16, 150)          3750      ['x19[0][0]']                 \n",
            "                                                                                                  \n",
            " x21 (BatchNormalization)    (None, 16, 16, 150)          600       ['x20[0][0]']                 \n",
            "                                                                                                  \n",
            " x22 (ReLU)                  (None, 16, 16, 150)          0         ['x21[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 16, 16, 150)          0         ['x22[0][0]']                 \n",
            "                                                                                                  \n",
            " x23 (DepthwiseConv2D)       (None, 16, 16, 150)          1500      ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " x24 (BatchNormalization)    (None, 16, 16, 150)          600       ['x23[0][0]']                 \n",
            "                                                                                                  \n",
            " x26 (ReLU)                  (None, 16, 16, 150)          0         ['x24[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 16, 16, 150)          0         ['x26[0][0]']                 \n",
            "                                                                                                  \n",
            " x27 (Conv2D)                (None, 16, 16, 24)           3624      ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " x28 (BatchNormalization)    (None, 16, 16, 24)           96        ['x27[0][0]']                 \n",
            "                                                                                                  \n",
            " x29 (Add)                   (None, 16, 16, 24)           0         ['x19[0][0]',                 \n",
            "                                                                     'x28[0][0]']                 \n",
            "                                                                                                  \n",
            " x30 (Conv2D)                (None, 16, 16, 150)          3750      ['x29[0][0]']                 \n",
            "                                                                                                  \n",
            " x31 (BatchNormalization)    (None, 16, 16, 150)          600       ['x30[0][0]']                 \n",
            "                                                                                                  \n",
            " x33 (ReLU)                  (None, 16, 16, 150)          0         ['x31[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 16, 16, 150)          0         ['x33[0][0]']                 \n",
            "                                                                                                  \n",
            " x34 (ZeroPadding2D)         (None, 18, 18, 150)          0         ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " x35 (DepthwiseConv2D)       (None, 8, 8, 150)            1500      ['x34[0][0]']                 \n",
            "                                                                                                  \n",
            " x36 (BatchNormalization)    (None, 8, 8, 150)            600       ['x35[0][0]']                 \n",
            "                                                                                                  \n",
            " x38 (ReLU)                  (None, 8, 8, 150)            0         ['x36[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 8, 8, 150)            0         ['x38[0][0]']                 \n",
            "                                                                                                  \n",
            " x39 (Conv2D)                (None, 8, 8, 64)             9664      ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " x40 (BatchNormalization)    (None, 8, 8, 64)             256       ['x39[0][0]']                 \n",
            "                                                                                                  \n",
            " x41 (Conv2D)                (None, 8, 8, 200)            13000     ['x40[0][0]']                 \n",
            "                                                                                                  \n",
            " x42 (BatchNormalization)    (None, 8, 8, 200)            800       ['x41[0][0]']                 \n",
            "                                                                                                  \n",
            " x44 (ReLU)                  (None, 8, 8, 200)            0         ['x42[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 8, 8, 200)            0         ['x44[0][0]']                 \n",
            "                                                                                                  \n",
            " x45 (DepthwiseConv2D)       (None, 8, 8, 200)            2000      ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " x46 (BatchNormalization)    (None, 8, 8, 200)            800       ['x45[0][0]']                 \n",
            "                                                                                                  \n",
            " x48 (ReLU)                  (None, 8, 8, 200)            0         ['x46[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 8, 8, 200)            0         ['x48[0][0]']                 \n",
            "                                                                                                  \n",
            " x49 (Conv2D)                (None, 8, 8, 64)             12864     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " x50 (BatchNormalization)    (None, 8, 8, 64)             256       ['x49[0][0]']                 \n",
            "                                                                                                  \n",
            " x51 (Add)                   (None, 8, 8, 64)             0         ['x40[0][0]',                 \n",
            "                                                                     'x50[0][0]']                 \n",
            "                                                                                                  \n",
            " x52 (Conv2D)                (None, 8, 8, 200)            13000     ['x51[0][0]']                 \n",
            "                                                                                                  \n",
            " x53 (BatchNormalization)    (None, 8, 8, 200)            800       ['x52[0][0]']                 \n",
            "                                                                                                  \n",
            " x55 (ReLU)                  (None, 8, 8, 200)            0         ['x53[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 8, 8, 200)            0         ['x55[0][0]']                 \n",
            "                                                                                                  \n",
            " x56 (DepthwiseConv2D)       (None, 8, 8, 200)            2000      ['dropout_9[0][0]']           \n",
            "                                                                                                  \n",
            " x57 (BatchNormalization)    (None, 8, 8, 200)            800       ['x56[0][0]']                 \n",
            "                                                                                                  \n",
            " x59 (ReLU)                  (None, 8, 8, 200)            0         ['x57[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 8, 8, 200)            0         ['x59[0][0]']                 \n",
            "                                                                                                  \n",
            " x60 (Conv2D)                (None, 8, 8, 64)             12864     ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " x61 (BatchNormalization)    (None, 8, 8, 64)             256       ['x60[0][0]']                 \n",
            "                                                                                                  \n",
            " x62 (Add)                   (None, 8, 8, 64)             0         ['x51[0][0]',                 \n",
            "                                                                     'x61[0][0]']                 \n",
            "                                                                                                  \n",
            " x63 (Conv2D)                (None, 8, 8, 200)            13000     ['x62[0][0]']                 \n",
            "                                                                                                  \n",
            " x64 (BatchNormalization)    (None, 8, 8, 200)            800       ['x63[0][0]']                 \n",
            "                                                                                                  \n",
            " x66 (ReLU)                  (None, 8, 8, 200)            0         ['x64[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 8, 8, 200)            0         ['x66[0][0]']                 \n",
            "                                                                                                  \n",
            " x67 (ZeroPadding2D)         (None, 10, 10, 200)          0         ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " x68 (DepthwiseConv2D)       (None, 4, 4, 200)            2000      ['x67[0][0]']                 \n",
            "                                                                                                  \n",
            " x69 (BatchNormalization)    (None, 4, 4, 200)            800       ['x68[0][0]']                 \n",
            "                                                                                                  \n",
            " x71 (ReLU)                  (None, 4, 4, 200)            0         ['x69[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 4, 4, 200)            0         ['x71[0][0]']                 \n",
            "                                                                                                  \n",
            " x72 (Conv2D)                (None, 4, 4, 64)             12864     ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " x73 (BatchNormalization)    (None, 4, 4, 64)             256       ['x72[0][0]']                 \n",
            "                                                                                                  \n",
            " x74 (Conv2D)                (None, 4, 4, 400)            26000     ['x73[0][0]']                 \n",
            "                                                                                                  \n",
            " x75 (BatchNormalization)    (None, 4, 4, 400)            1600      ['x74[0][0]']                 \n",
            "                                                                                                  \n",
            " x77 (ReLU)                  (None, 4, 4, 400)            0         ['x75[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 4, 4, 400)            0         ['x77[0][0]']                 \n",
            "                                                                                                  \n",
            " x78 (DepthwiseConv2D)       (None, 4, 4, 400)            4000      ['dropout_13[0][0]']          \n",
            "                                                                                                  \n",
            " x79 (BatchNormalization)    (None, 4, 4, 400)            1600      ['x78[0][0]']                 \n",
            "                                                                                                  \n",
            " x81 (ReLU)                  (None, 4, 4, 400)            0         ['x79[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 4, 4, 400)            0         ['x81[0][0]']                 \n",
            "                                                                                                  \n",
            " x82 (Conv2D)                (None, 4, 4, 64)             25664     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " x83 (BatchNormalization)    (None, 4, 4, 64)             256       ['x82[0][0]']                 \n",
            "                                                                                                  \n",
            " x84 (Add)                   (None, 4, 4, 64)             0         ['x73[0][0]',                 \n",
            "                                                                     'x83[0][0]']                 \n",
            "                                                                                                  \n",
            " x85 (Conv2D)                (None, 4, 4, 400)            26000     ['x84[0][0]']                 \n",
            "                                                                                                  \n",
            " x86 (BatchNormalization)    (None, 4, 4, 400)            1600      ['x85[0][0]']                 \n",
            "                                                                                                  \n",
            " x88 (ReLU)                  (None, 4, 4, 400)            0         ['x86[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 4, 4, 400)            0         ['x88[0][0]']                 \n",
            "                                                                                                  \n",
            " x89 (DepthwiseConv2D)       (None, 4, 4, 400)            4000      ['dropout_15[0][0]']          \n",
            "                                                                                                  \n",
            " x90 (BatchNormalization)    (None, 4, 4, 400)            1600      ['x89[0][0]']                 \n",
            "                                                                                                  \n",
            " x92 (ReLU)                  (None, 4, 4, 400)            0         ['x90[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 4, 4, 400)            0         ['x92[0][0]']                 \n",
            "                                                                                                  \n",
            " x93 (Conv2D)                (None, 4, 4, 64)             25664     ['dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " x94 (BatchNormalization)    (None, 4, 4, 64)             256       ['x93[0][0]']                 \n",
            "                                                                                                  \n",
            " x95 (Add)                   (None, 4, 4, 64)             0         ['x84[0][0]',                 \n",
            "                                                                     'x94[0][0]']                 \n",
            "                                                                                                  \n",
            " x96 (Conv2D)                (None, 4, 4, 384)            24960     ['x95[0][0]']                 \n",
            "                                                                                                  \n",
            " x97 (BatchNormalization)    (None, 4, 4, 384)            1536      ['x96[0][0]']                 \n",
            "                                                                                                  \n",
            " x99 (ReLU)                  (None, 4, 4, 384)            0         ['x97[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 4, 4, 384)            0         ['x99[0][0]']                 \n",
            "                                                                                                  \n",
            " x100 (DepthwiseConv2D)      (None, 4, 4, 384)            3840      ['dropout_17[0][0]']          \n",
            "                                                                                                  \n",
            " x101 (BatchNormalization)   (None, 4, 4, 384)            1536      ['x100[0][0]']                \n",
            "                                                                                                  \n",
            " x103 (ReLU)                 (None, 4, 4, 384)            0         ['x101[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, 4, 4, 384)            0         ['x103[0][0]']                \n",
            "                                                                                                  \n",
            " x104 (Conv2D)               (None, 4, 4, 64)             24640     ['dropout_18[0][0]']          \n",
            "                                                                                                  \n",
            " x105 (BatchNormalization)   (None, 4, 4, 64)             256       ['x104[0][0]']                \n",
            "                                                                                                  \n",
            " x106 (Add)                  (None, 4, 4, 64)             0         ['x95[0][0]',                 \n",
            "                                                                     'x105[0][0]']                \n",
            "                                                                                                  \n",
            " x107 (Conv2D)               (None, 4, 4, 384)            24960     ['x106[0][0]']                \n",
            "                                                                                                  \n",
            " x108 (BatchNormalization)   (None, 4, 4, 384)            1536      ['x107[0][0]']                \n",
            "                                                                                                  \n",
            " x110 (ReLU)                 (None, 4, 4, 384)            0         ['x108[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)        (None, 4, 4, 384)            0         ['x110[0][0]']                \n",
            "                                                                                                  \n",
            " x111 (DepthwiseConv2D)      (None, 4, 4, 384)            3840      ['dropout_19[0][0]']          \n",
            "                                                                                                  \n",
            " x112 (BatchNormalization)   (None, 4, 4, 384)            1536      ['x111[0][0]']                \n",
            "                                                                                                  \n",
            " x114 (ReLU)                 (None, 4, 4, 384)            0         ['x112[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)        (None, 4, 4, 384)            0         ['x114[0][0]']                \n",
            "                                                                                                  \n",
            " x115 (Conv2D)               (None, 4, 4, 96)             36960     ['dropout_20[0][0]']          \n",
            "                                                                                                  \n",
            " x116 (BatchNormalization)   (None, 4, 4, 96)             384       ['x115[0][0]']                \n",
            "                                                                                                  \n",
            " x117 (Conv2D)               (None, 4, 4, 576)            55872     ['x116[0][0]']                \n",
            "                                                                                                  \n",
            " x118 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x117[0][0]']                \n",
            "                                                                                                  \n",
            " x120 (ReLU)                 (None, 4, 4, 576)            0         ['x118[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)        (None, 4, 4, 576)            0         ['x120[0][0]']                \n",
            "                                                                                                  \n",
            " x121 (DepthwiseConv2D)      (None, 4, 4, 576)            5760      ['dropout_21[0][0]']          \n",
            "                                                                                                  \n",
            " x122 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x121[0][0]']                \n",
            "                                                                                                  \n",
            " x124 (ReLU)                 (None, 4, 4, 576)            0         ['x122[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)        (None, 4, 4, 576)            0         ['x124[0][0]']                \n",
            "                                                                                                  \n",
            " x125 (Conv2D)               (None, 4, 4, 96)             55392     ['dropout_22[0][0]']          \n",
            "                                                                                                  \n",
            " x126 (BatchNormalization)   (None, 4, 4, 96)             384       ['x125[0][0]']                \n",
            "                                                                                                  \n",
            " x127 (Add)                  (None, 4, 4, 96)             0         ['x116[0][0]',                \n",
            "                                                                     'x126[0][0]']                \n",
            "                                                                                                  \n",
            " x128 (Conv2D)               (None, 4, 4, 576)            55872     ['x127[0][0]']                \n",
            "                                                                                                  \n",
            " x129 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x128[0][0]']                \n",
            "                                                                                                  \n",
            " x131 (ReLU)                 (None, 4, 4, 576)            0         ['x129[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)        (None, 4, 4, 576)            0         ['x131[0][0]']                \n",
            "                                                                                                  \n",
            " x132 (DepthwiseConv2D)      (None, 4, 4, 576)            5760      ['dropout_23[0][0]']          \n",
            "                                                                                                  \n",
            " x133 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x132[0][0]']                \n",
            "                                                                                                  \n",
            " x135 (ReLU)                 (None, 4, 4, 576)            0         ['x133[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)        (None, 4, 4, 576)            0         ['x135[0][0]']                \n",
            "                                                                                                  \n",
            " x136 (Conv2D)               (None, 4, 4, 96)             55392     ['dropout_24[0][0]']          \n",
            "                                                                                                  \n",
            " x137 (BatchNormalization)   (None, 4, 4, 96)             384       ['x136[0][0]']                \n",
            "                                                                                                  \n",
            " x138 (Add)                  (None, 4, 4, 96)             0         ['x127[0][0]',                \n",
            "                                                                     'x137[0][0]']                \n",
            "                                                                                                  \n",
            " x128_ (Conv2D)              (None, 4, 4, 576)            55872     ['x138[0][0]']                \n",
            "                                                                                                  \n",
            " x129_ (BatchNormalization)  (None, 4, 4, 576)            2304      ['x128_[0][0]']               \n",
            "                                                                                                  \n",
            " x131_ (ReLU)                (None, 4, 4, 576)            0         ['x129_[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 4, 4, 576)            0         ['x131_[0][0]']               \n",
            "                                                                                                  \n",
            " x132_ (DepthwiseConv2D)     (None, 4, 4, 576)            5760      ['dropout_25[0][0]']          \n",
            "                                                                                                  \n",
            " x133_ (BatchNormalization)  (None, 4, 4, 576)            2304      ['x132_[0][0]']               \n",
            "                                                                                                  \n",
            " x135_ (ReLU)                (None, 4, 4, 576)            0         ['x133_[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 4, 4, 576)            0         ['x135_[0][0]']               \n",
            "                                                                                                  \n",
            " x136_ (Conv2D)              (None, 4, 4, 96)             55392     ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " x137_ (BatchNormalization)  (None, 4, 4, 96)             384       ['x136_[0][0]']               \n",
            "                                                                                                  \n",
            " x138_ (Add)                 (None, 4, 4, 96)             0         ['x127[0][0]',                \n",
            "                                                                     'x137_[0][0]']               \n",
            "                                                                                                  \n",
            " x139 (Conv2D)               (None, 4, 4, 576)            55872     ['x138_[0][0]']               \n",
            "                                                                                                  \n",
            " x140 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x139[0][0]']                \n",
            "                                                                                                  \n",
            " x142 (ReLU)                 (None, 4, 4, 576)            0         ['x140[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 4, 4, 576)            0         ['x142[0][0]']                \n",
            "                                                                                                  \n",
            " x143 (ZeroPadding2D)        (None, 6, 6, 576)            0         ['dropout_27[0][0]']          \n",
            "                                                                                                  \n",
            " x144 (DepthwiseConv2D)      (None, 4, 4, 576)            5760      ['x143[0][0]']                \n",
            "                                                                                                  \n",
            " x145 (BatchNormalization)   (None, 4, 4, 576)            2304      ['x144[0][0]']                \n",
            "                                                                                                  \n",
            " x147 (ReLU)                 (None, 4, 4, 576)            0         ['x145[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 4, 4, 576)            0         ['x147[0][0]']                \n",
            "                                                                                                  \n",
            " x148 (Conv2D)               (None, 4, 4, 160)            92320     ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " x149 (BatchNormalization)   (None, 4, 4, 160)            640       ['x148[0][0]']                \n",
            "                                                                                                  \n",
            " x150 (Conv2D)               (None, 4, 4, 960)            154560    ['x149[0][0]']                \n",
            "                                                                                                  \n",
            " x151 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x150[0][0]']                \n",
            "                                                                                                  \n",
            " x153 (ReLU)                 (None, 4, 4, 960)            0         ['x151[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 4, 4, 960)            0         ['x153[0][0]']                \n",
            "                                                                                                  \n",
            " x154 (DepthwiseConv2D)      (None, 4, 4, 960)            9600      ['dropout_29[0][0]']          \n",
            "                                                                                                  \n",
            " x155 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x154[0][0]']                \n",
            "                                                                                                  \n",
            " x157 (ReLU)                 (None, 4, 4, 960)            0         ['x155[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)        (None, 4, 4, 960)            0         ['x157[0][0]']                \n",
            "                                                                                                  \n",
            " x158 (Conv2D)               (None, 4, 4, 160)            153760    ['dropout_30[0][0]']          \n",
            "                                                                                                  \n",
            " x159 (BatchNormalization)   (None, 4, 4, 160)            640       ['x158[0][0]']                \n",
            "                                                                                                  \n",
            " x160 (Add)                  (None, 4, 4, 160)            0         ['x149[0][0]',                \n",
            "                                                                     'x159[0][0]']                \n",
            "                                                                                                  \n",
            " x161 (Conv2D)               (None, 4, 4, 960)            154560    ['x160[0][0]']                \n",
            "                                                                                                  \n",
            " x162 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x161[0][0]']                \n",
            "                                                                                                  \n",
            " x164 (ReLU)                 (None, 4, 4, 960)            0         ['x162[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)        (None, 4, 4, 960)            0         ['x164[0][0]']                \n",
            "                                                                                                  \n",
            " x165 (DepthwiseConv2D)      (None, 4, 4, 960)            9600      ['dropout_31[0][0]']          \n",
            "                                                                                                  \n",
            " x166 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x165[0][0]']                \n",
            "                                                                                                  \n",
            " x168 (ReLU)                 (None, 4, 4, 960)            0         ['x166[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)        (None, 4, 4, 960)            0         ['x168[0][0]']                \n",
            "                                                                                                  \n",
            " x169 (Conv2D)               (None, 4, 4, 160)            153760    ['dropout_32[0][0]']          \n",
            "                                                                                                  \n",
            " x170 (BatchNormalization)   (None, 4, 4, 160)            640       ['x169[0][0]']                \n",
            "                                                                                                  \n",
            " x171 (Add)                  (None, 4, 4, 160)            0         ['x160[0][0]',                \n",
            "                                                                     'x170[0][0]']                \n",
            "                                                                                                  \n",
            " x172 (Conv2D)               (None, 4, 4, 960)            154560    ['x171[0][0]']                \n",
            "                                                                                                  \n",
            " x173 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x172[0][0]']                \n",
            "                                                                                                  \n",
            " x175 (ReLU)                 (None, 4, 4, 960)            0         ['x173[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)        (None, 4, 4, 960)            0         ['x175[0][0]']                \n",
            "                                                                                                  \n",
            " x176 (DepthwiseConv2D)      (None, 4, 4, 960)            9600      ['dropout_33[0][0]']          \n",
            "                                                                                                  \n",
            " x177 (BatchNormalization)   (None, 4, 4, 960)            3840      ['x176[0][0]']                \n",
            "                                                                                                  \n",
            " x179 (ReLU)                 (None, 4, 4, 960)            0         ['x177[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)        (None, 4, 4, 960)            0         ['x179[0][0]']                \n",
            "                                                                                                  \n",
            " x180 (Conv2D)               (None, 4, 4, 320)            307520    ['dropout_34[0][0]']          \n",
            "                                                                                                  \n",
            " x181 (BatchNormalization)   (None, 4, 4, 320)            1280      ['x180[0][0]']                \n",
            "                                                                                                  \n",
            " x182 (Conv2D)               (None, 4, 4, 1280)           410880    ['x181[0][0]']                \n",
            "                                                                                                  \n",
            " x183 (BatchNormalization)   (None, 4, 4, 1280)           5120      ['x182[0][0]']                \n",
            "                                                                                                  \n",
            " out_put (ReLU)              (None, 4, 4, 1280)           0         ['x183[0][0]']                \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 1280)                 0         ['out_put[0][0]']             \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  163968    ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 10)                   1290      ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2636598 (10.06 MB)\n",
            "Trainable params: 2599366 (9.92 MB)\n",
            "Non-trainable params: 37232 (145.44 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some helper functions"
      ],
      "metadata": {
        "id": "YtK0DZhvR_V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=80, restore_best_weights=True)\n",
        "\n",
        "# Define model checkpoint callback\n",
        "model_checkpoint = ModelCheckpoint('best_model_IsaT_6.h5',\n",
        "                                   monitor='val_loss',\n",
        "                                   save_best_only=True,\n",
        "                                   mode='min',\n",
        "                                   verbose=1)"
      ],
      "metadata": {
        "id": "qSW-vZ3LR-gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rate schedule for SGD optimizer\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
        "\n",
        "# Create SGD optimizer with the learning rate schedule\n",
        "sgd_optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "                optimizer= sgd_optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(X_train, y_train, epochs=1000,batch_size = 128, validation_data=(X_val, y_val), callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YINfqOn_SgU8",
        "outputId": "3de64161-c4f5-45a7-97e1-e9ea53100e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 2.5575 - accuracy: 0.4844\n",
            "Epoch 1: val_loss improved from inf to 2.20006, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 53s 108ms/step - loss: 2.5575 - accuracy: 0.4844 - val_loss: 2.2001 - val_accuracy: 0.4110\n",
            "Epoch 2/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.6153 - accuracy: 0.5273\n",
            "Epoch 2: val_loss improved from 2.20006 to 1.83931, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 1.6153 - accuracy: 0.5273 - val_loss: 1.8393 - val_accuracy: 0.4112\n",
            "Epoch 3/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.3172 - accuracy: 0.5681\n",
            "Epoch 3: val_loss improved from 1.83931 to 1.64447, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 1.3172 - accuracy: 0.5681 - val_loss: 1.6445 - val_accuracy: 0.4738\n",
            "Epoch 4/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.1998 - accuracy: 0.5963\n",
            "Epoch 4: val_loss improved from 1.64447 to 1.53544, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 1.1998 - accuracy: 0.5963 - val_loss: 1.5354 - val_accuracy: 0.5218\n",
            "Epoch 5/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.1252 - accuracy: 0.6225\n",
            "Epoch 5: val_loss did not improve from 1.53544\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 1.1252 - accuracy: 0.6225 - val_loss: 1.5480 - val_accuracy: 0.5038\n",
            "Epoch 6/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.6405\n",
            "Epoch 6: val_loss improved from 1.53544 to 1.26053, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 1.0734 - accuracy: 0.6405 - val_loss: 1.2605 - val_accuracy: 0.5874\n",
            "Epoch 7/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 1.0300 - accuracy: 0.6573\n",
            "Epoch 7: val_loss did not improve from 1.26053\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 1.0300 - accuracy: 0.6573 - val_loss: 1.3290 - val_accuracy: 0.5764\n",
            "Epoch 8/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.6671\n",
            "Epoch 8: val_loss improved from 1.26053 to 1.20368, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.9989 - accuracy: 0.6671 - val_loss: 1.2037 - val_accuracy: 0.6188\n",
            "Epoch 9/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.9701 - accuracy: 0.6780\n",
            "Epoch 9: val_loss improved from 1.20368 to 1.09160, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.9701 - accuracy: 0.6780 - val_loss: 1.0916 - val_accuracy: 0.6494\n",
            "Epoch 10/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.9403 - accuracy: 0.6882\n",
            "Epoch 10: val_loss did not improve from 1.09160\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.9403 - accuracy: 0.6882 - val_loss: 1.4282 - val_accuracy: 0.5672\n",
            "Epoch 11/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.6973\n",
            "Epoch 11: val_loss improved from 1.09160 to 1.00527, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.9120 - accuracy: 0.6973 - val_loss: 1.0053 - val_accuracy: 0.6790\n",
            "Epoch 12/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.8912 - accuracy: 0.7083\n",
            "Epoch 12: val_loss improved from 1.00527 to 1.00260, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.8912 - accuracy: 0.7083 - val_loss: 1.0026 - val_accuracy: 0.6658\n",
            "Epoch 13/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.7129\n",
            "Epoch 13: val_loss improved from 1.00260 to 0.98197, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.8699 - accuracy: 0.7129 - val_loss: 0.9820 - val_accuracy: 0.6760\n",
            "Epoch 14/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.8534 - accuracy: 0.7226\n",
            "Epoch 14: val_loss did not improve from 0.98197\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.8534 - accuracy: 0.7226 - val_loss: 1.0389 - val_accuracy: 0.6744\n",
            "Epoch 15/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.8285 - accuracy: 0.7298\n",
            "Epoch 15: val_loss improved from 0.98197 to 0.96367, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.8285 - accuracy: 0.7298 - val_loss: 0.9637 - val_accuracy: 0.6848\n",
            "Epoch 16/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.7361\n",
            "Epoch 16: val_loss improved from 0.96367 to 0.88868, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.8146 - accuracy: 0.7361 - val_loss: 0.8887 - val_accuracy: 0.7184\n",
            "Epoch 17/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7987 - accuracy: 0.7412\n",
            "Epoch 17: val_loss did not improve from 0.88868\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.7987 - accuracy: 0.7412 - val_loss: 0.9356 - val_accuracy: 0.6970\n",
            "Epoch 18/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7835 - accuracy: 0.7456\n",
            "Epoch 18: val_loss did not improve from 0.88868\n",
            "352/352 [==============================] - 38s 108ms/step - loss: 0.7835 - accuracy: 0.7456 - val_loss: 0.9722 - val_accuracy: 0.6872\n",
            "Epoch 19/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7607 - accuracy: 0.7516\n",
            "Epoch 19: val_loss did not improve from 0.88868\n",
            "352/352 [==============================] - 40s 114ms/step - loss: 0.7607 - accuracy: 0.7516 - val_loss: 0.9600 - val_accuracy: 0.6978\n",
            "Epoch 20/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7493 - accuracy: 0.7586\n",
            "Epoch 20: val_loss improved from 0.88868 to 0.84288, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 41s 116ms/step - loss: 0.7493 - accuracy: 0.7586 - val_loss: 0.8429 - val_accuracy: 0.7216\n",
            "Epoch 21/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7377 - accuracy: 0.7591\n",
            "Epoch 21: val_loss did not improve from 0.84288\n",
            "352/352 [==============================] - 39s 111ms/step - loss: 0.7377 - accuracy: 0.7591 - val_loss: 0.9192 - val_accuracy: 0.7094\n",
            "Epoch 22/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7269 - accuracy: 0.7653\n",
            "Epoch 22: val_loss did not improve from 0.84288\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.7269 - accuracy: 0.7653 - val_loss: 0.8679 - val_accuracy: 0.7242\n",
            "Epoch 23/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.7703\n",
            "Epoch 23: val_loss did not improve from 0.84288\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.7192 - accuracy: 0.7703 - val_loss: 0.9413 - val_accuracy: 0.7154\n",
            "Epoch 24/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.7738\n",
            "Epoch 24: val_loss improved from 0.84288 to 0.78484, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 39s 111ms/step - loss: 0.7022 - accuracy: 0.7738 - val_loss: 0.7848 - val_accuracy: 0.7510\n",
            "Epoch 25/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.7774\n",
            "Epoch 25: val_loss did not improve from 0.78484\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.6921 - accuracy: 0.7774 - val_loss: 0.7902 - val_accuracy: 0.7476\n",
            "Epoch 26/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6787 - accuracy: 0.7802\n",
            "Epoch 26: val_loss did not improve from 0.78484\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 0.6787 - accuracy: 0.7802 - val_loss: 0.8913 - val_accuracy: 0.7198\n",
            "Epoch 27/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.7867\n",
            "Epoch 27: val_loss improved from 0.78484 to 0.77389, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.6639 - accuracy: 0.7867 - val_loss: 0.7739 - val_accuracy: 0.7614\n",
            "Epoch 28/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.7901\n",
            "Epoch 28: val_loss improved from 0.77389 to 0.73250, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.6573 - accuracy: 0.7901 - val_loss: 0.7325 - val_accuracy: 0.7648\n",
            "Epoch 29/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.7952\n",
            "Epoch 29: val_loss did not improve from 0.73250\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.6420 - accuracy: 0.7952 - val_loss: 0.9669 - val_accuracy: 0.7118\n",
            "Epoch 30/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.7975\n",
            "Epoch 30: val_loss did not improve from 0.73250\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.6322 - accuracy: 0.7975 - val_loss: 0.8911 - val_accuracy: 0.7254\n",
            "Epoch 31/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.8023\n",
            "Epoch 31: val_loss improved from 0.73250 to 0.67466, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.6162 - accuracy: 0.8023 - val_loss: 0.6747 - val_accuracy: 0.7786\n",
            "Epoch 32/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.8046\n",
            "Epoch 32: val_loss did not improve from 0.67466\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.6104 - accuracy: 0.8046 - val_loss: 0.7996 - val_accuracy: 0.7462\n",
            "Epoch 33/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.8081\n",
            "Epoch 33: val_loss did not improve from 0.67466\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.6037 - accuracy: 0.8081 - val_loss: 0.8953 - val_accuracy: 0.7364\n",
            "Epoch 34/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.8093\n",
            "Epoch 34: val_loss did not improve from 0.67466\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5981 - accuracy: 0.8093 - val_loss: 0.7131 - val_accuracy: 0.7752\n",
            "Epoch 35/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.8115\n",
            "Epoch 35: val_loss did not improve from 0.67466\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.5931 - accuracy: 0.8115 - val_loss: 0.7668 - val_accuracy: 0.7678\n",
            "Epoch 36/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5799 - accuracy: 0.8148\n",
            "Epoch 36: val_loss improved from 0.67466 to 0.67034, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.5799 - accuracy: 0.8148 - val_loss: 0.6703 - val_accuracy: 0.7866\n",
            "Epoch 37/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.8170\n",
            "Epoch 37: val_loss did not improve from 0.67034\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.5771 - accuracy: 0.8170 - val_loss: 0.7389 - val_accuracy: 0.7710\n",
            "Epoch 38/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.8190\n",
            "Epoch 38: val_loss improved from 0.67034 to 0.60680, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5664 - accuracy: 0.8190 - val_loss: 0.6068 - val_accuracy: 0.8102\n",
            "Epoch 39/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.8193\n",
            "Epoch 39: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 104ms/step - loss: 0.5634 - accuracy: 0.8193 - val_loss: 0.7885 - val_accuracy: 0.7592\n",
            "Epoch 40/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.8242\n",
            "Epoch 40: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5533 - accuracy: 0.8242 - val_loss: 0.6630 - val_accuracy: 0.7890\n",
            "Epoch 41/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.8260\n",
            "Epoch 41: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 104ms/step - loss: 0.5496 - accuracy: 0.8260 - val_loss: 0.8410 - val_accuracy: 0.7438\n",
            "Epoch 42/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.8259\n",
            "Epoch 42: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.5465 - accuracy: 0.8259 - val_loss: 0.7255 - val_accuracy: 0.7798\n",
            "Epoch 43/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.8276\n",
            "Epoch 43: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.5420 - accuracy: 0.8276 - val_loss: 0.8604 - val_accuracy: 0.7442\n",
            "Epoch 44/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.8305\n",
            "Epoch 44: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 104ms/step - loss: 0.5370 - accuracy: 0.8305 - val_loss: 0.8818 - val_accuracy: 0.7480\n",
            "Epoch 45/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.8314\n",
            "Epoch 45: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 0.5276 - accuracy: 0.8314 - val_loss: 0.6157 - val_accuracy: 0.8046\n",
            "Epoch 46/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.8337\n",
            "Epoch 46: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 104ms/step - loss: 0.5223 - accuracy: 0.8337 - val_loss: 0.6942 - val_accuracy: 0.7848\n",
            "Epoch 47/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8374\n",
            "Epoch 47: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5173 - accuracy: 0.8374 - val_loss: 0.6805 - val_accuracy: 0.7852\n",
            "Epoch 48/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.8352\n",
            "Epoch 48: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5155 - accuracy: 0.8352 - val_loss: 0.7224 - val_accuracy: 0.7770\n",
            "Epoch 49/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.8404\n",
            "Epoch 49: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 36s 104ms/step - loss: 0.5070 - accuracy: 0.8404 - val_loss: 0.7201 - val_accuracy: 0.7728\n",
            "Epoch 50/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.8385\n",
            "Epoch 50: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5081 - accuracy: 0.8385 - val_loss: 0.6729 - val_accuracy: 0.7912\n",
            "Epoch 51/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8404\n",
            "Epoch 51: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.5023 - accuracy: 0.8404 - val_loss: 0.6502 - val_accuracy: 0.8008\n",
            "Epoch 52/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.8441\n",
            "Epoch 52: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4909 - accuracy: 0.8441 - val_loss: 0.6722 - val_accuracy: 0.7932\n",
            "Epoch 53/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.8439\n",
            "Epoch 53: val_loss did not improve from 0.60680\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4919 - accuracy: 0.8439 - val_loss: 0.6731 - val_accuracy: 0.7934\n",
            "Epoch 54/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.8473\n",
            "Epoch 54: val_loss improved from 0.60680 to 0.54893, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.4857 - accuracy: 0.8473 - val_loss: 0.5489 - val_accuracy: 0.8292\n",
            "Epoch 55/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8505\n",
            "Epoch 55: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.4801 - accuracy: 0.8505 - val_loss: 0.6264 - val_accuracy: 0.8022\n",
            "Epoch 56/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.8474\n",
            "Epoch 56: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4816 - accuracy: 0.8474 - val_loss: 0.5952 - val_accuracy: 0.8158\n",
            "Epoch 57/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.8508\n",
            "Epoch 57: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4728 - accuracy: 0.8508 - val_loss: 0.5679 - val_accuracy: 0.8218\n",
            "Epoch 58/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8532\n",
            "Epoch 58: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4634 - accuracy: 0.8532 - val_loss: 0.6169 - val_accuracy: 0.8154\n",
            "Epoch 59/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.8581\n",
            "Epoch 59: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4528 - accuracy: 0.8581 - val_loss: 0.6117 - val_accuracy: 0.8108\n",
            "Epoch 60/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.8567\n",
            "Epoch 60: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4538 - accuracy: 0.8567 - val_loss: 0.6194 - val_accuracy: 0.8136\n",
            "Epoch 61/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.8590\n",
            "Epoch 61: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.4462 - accuracy: 0.8590 - val_loss: 0.5903 - val_accuracy: 0.8106\n",
            "Epoch 62/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.8577\n",
            "Epoch 62: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4490 - accuracy: 0.8577 - val_loss: 0.6091 - val_accuracy: 0.8078\n",
            "Epoch 63/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.8614\n",
            "Epoch 63: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4368 - accuracy: 0.8614 - val_loss: 0.6457 - val_accuracy: 0.8014\n",
            "Epoch 64/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.8623\n",
            "Epoch 64: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4388 - accuracy: 0.8623 - val_loss: 0.6593 - val_accuracy: 0.8070\n",
            "Epoch 65/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8610\n",
            "Epoch 65: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4376 - accuracy: 0.8610 - val_loss: 0.6914 - val_accuracy: 0.7922\n",
            "Epoch 66/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8640\n",
            "Epoch 66: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4336 - accuracy: 0.8640 - val_loss: 0.5526 - val_accuracy: 0.8262\n",
            "Epoch 67/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8650\n",
            "Epoch 67: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4283 - accuracy: 0.8650 - val_loss: 0.6884 - val_accuracy: 0.7966\n",
            "Epoch 68/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8673\n",
            "Epoch 68: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.4268 - accuracy: 0.8673 - val_loss: 0.5638 - val_accuracy: 0.8198\n",
            "Epoch 69/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.8638\n",
            "Epoch 69: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.4266 - accuracy: 0.8638 - val_loss: 0.6696 - val_accuracy: 0.8026\n",
            "Epoch 70/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.8670\n",
            "Epoch 70: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4212 - accuracy: 0.8670 - val_loss: 0.5628 - val_accuracy: 0.8306\n",
            "Epoch 71/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8675\n",
            "Epoch 71: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4236 - accuracy: 0.8675 - val_loss: 0.6199 - val_accuracy: 0.8132\n",
            "Epoch 72/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.8678\n",
            "Epoch 72: val_loss did not improve from 0.54893\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4193 - accuracy: 0.8678 - val_loss: 0.5713 - val_accuracy: 0.8270\n",
            "Epoch 73/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.8708\n",
            "Epoch 73: val_loss improved from 0.54893 to 0.52149, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.4130 - accuracy: 0.8708 - val_loss: 0.5215 - val_accuracy: 0.8390\n",
            "Epoch 74/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.8709\n",
            "Epoch 74: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4123 - accuracy: 0.8709 - val_loss: 0.6293 - val_accuracy: 0.8072\n",
            "Epoch 75/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.8708\n",
            "Epoch 75: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.4106 - accuracy: 0.8708 - val_loss: 0.5445 - val_accuracy: 0.8328\n",
            "Epoch 76/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.8714\n",
            "Epoch 76: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.4078 - accuracy: 0.8714 - val_loss: 0.7149 - val_accuracy: 0.7858\n",
            "Epoch 77/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.8726\n",
            "Epoch 77: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.4078 - accuracy: 0.8726 - val_loss: 0.5412 - val_accuracy: 0.8362\n",
            "Epoch 78/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.8758\n",
            "Epoch 78: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3978 - accuracy: 0.8758 - val_loss: 0.5632 - val_accuracy: 0.8296\n",
            "Epoch 79/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.8736\n",
            "Epoch 79: val_loss did not improve from 0.52149\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3965 - accuracy: 0.8736 - val_loss: 0.5590 - val_accuracy: 0.8266\n",
            "Epoch 80/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8745\n",
            "Epoch 80: val_loss improved from 0.52149 to 0.51817, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3984 - accuracy: 0.8745 - val_loss: 0.5182 - val_accuracy: 0.8380\n",
            "Epoch 81/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.8765\n",
            "Epoch 81: val_loss did not improve from 0.51817\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3920 - accuracy: 0.8765 - val_loss: 0.5869 - val_accuracy: 0.8168\n",
            "Epoch 82/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8770\n",
            "Epoch 82: val_loss improved from 0.51817 to 0.50902, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.3924 - accuracy: 0.8770 - val_loss: 0.5090 - val_accuracy: 0.8442\n",
            "Epoch 83/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.8795\n",
            "Epoch 83: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3882 - accuracy: 0.8795 - val_loss: 0.5473 - val_accuracy: 0.8328\n",
            "Epoch 84/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.8819\n",
            "Epoch 84: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3790 - accuracy: 0.8819 - val_loss: 0.5572 - val_accuracy: 0.8286\n",
            "Epoch 85/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8797\n",
            "Epoch 85: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3810 - accuracy: 0.8797 - val_loss: 0.5760 - val_accuracy: 0.8252\n",
            "Epoch 86/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8823\n",
            "Epoch 86: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3757 - accuracy: 0.8823 - val_loss: 0.5303 - val_accuracy: 0.8384\n",
            "Epoch 87/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8810\n",
            "Epoch 87: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3732 - accuracy: 0.8810 - val_loss: 0.5782 - val_accuracy: 0.8278\n",
            "Epoch 88/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.8841\n",
            "Epoch 88: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3661 - accuracy: 0.8841 - val_loss: 0.5266 - val_accuracy: 0.8416\n",
            "Epoch 89/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.8861\n",
            "Epoch 89: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3721 - accuracy: 0.8861 - val_loss: 0.5217 - val_accuracy: 0.8416\n",
            "Epoch 90/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8850\n",
            "Epoch 90: val_loss did not improve from 0.50902\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3635 - accuracy: 0.8850 - val_loss: 0.5375 - val_accuracy: 0.8394\n",
            "Epoch 91/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3650 - accuracy: 0.8857\n",
            "Epoch 91: val_loss improved from 0.50902 to 0.49862, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3650 - accuracy: 0.8857 - val_loss: 0.4986 - val_accuracy: 0.8504\n",
            "Epoch 92/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8858\n",
            "Epoch 92: val_loss did not improve from 0.49862\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3632 - accuracy: 0.8858 - val_loss: 0.5042 - val_accuracy: 0.8486\n",
            "Epoch 93/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.8860\n",
            "Epoch 93: val_loss improved from 0.49862 to 0.49595, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 108ms/step - loss: 0.3651 - accuracy: 0.8860 - val_loss: 0.4960 - val_accuracy: 0.8500\n",
            "Epoch 94/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8887\n",
            "Epoch 94: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3541 - accuracy: 0.8887 - val_loss: 0.5735 - val_accuracy: 0.8334\n",
            "Epoch 95/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.8900\n",
            "Epoch 95: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3530 - accuracy: 0.8900 - val_loss: 0.5523 - val_accuracy: 0.8408\n",
            "Epoch 96/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.8880\n",
            "Epoch 96: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3582 - accuracy: 0.8880 - val_loss: 0.5163 - val_accuracy: 0.8432\n",
            "Epoch 97/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8897\n",
            "Epoch 97: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3497 - accuracy: 0.8897 - val_loss: 0.5132 - val_accuracy: 0.8432\n",
            "Epoch 98/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8914\n",
            "Epoch 98: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3459 - accuracy: 0.8914 - val_loss: 0.5380 - val_accuracy: 0.8452\n",
            "Epoch 99/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8934\n",
            "Epoch 99: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3378 - accuracy: 0.8934 - val_loss: 0.5109 - val_accuracy: 0.8462\n",
            "Epoch 100/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.8906\n",
            "Epoch 100: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3477 - accuracy: 0.8906 - val_loss: 0.4988 - val_accuracy: 0.8460\n",
            "Epoch 101/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8914\n",
            "Epoch 101: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3432 - accuracy: 0.8914 - val_loss: 0.5267 - val_accuracy: 0.8430\n",
            "Epoch 102/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8931\n",
            "Epoch 102: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3440 - accuracy: 0.8931 - val_loss: 0.5279 - val_accuracy: 0.8380\n",
            "Epoch 103/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8938\n",
            "Epoch 103: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3360 - accuracy: 0.8938 - val_loss: 0.5155 - val_accuracy: 0.8464\n",
            "Epoch 104/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.8922\n",
            "Epoch 104: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3422 - accuracy: 0.8922 - val_loss: 0.4971 - val_accuracy: 0.8488\n",
            "Epoch 105/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.8942\n",
            "Epoch 105: val_loss did not improve from 0.49595\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3385 - accuracy: 0.8942 - val_loss: 0.5445 - val_accuracy: 0.8400\n",
            "Epoch 106/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8943\n",
            "Epoch 106: val_loss improved from 0.49595 to 0.47754, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.3371 - accuracy: 0.8943 - val_loss: 0.4775 - val_accuracy: 0.8508\n",
            "Epoch 107/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8947\n",
            "Epoch 107: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3323 - accuracy: 0.8947 - val_loss: 0.5995 - val_accuracy: 0.8246\n",
            "Epoch 108/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8972\n",
            "Epoch 108: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3304 - accuracy: 0.8972 - val_loss: 0.5144 - val_accuracy: 0.8542\n",
            "Epoch 109/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8966\n",
            "Epoch 109: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3289 - accuracy: 0.8966 - val_loss: 0.5167 - val_accuracy: 0.8450\n",
            "Epoch 110/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.8976\n",
            "Epoch 110: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3268 - accuracy: 0.8976 - val_loss: 0.5143 - val_accuracy: 0.8436\n",
            "Epoch 111/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8988\n",
            "Epoch 111: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3264 - accuracy: 0.8988 - val_loss: 0.5410 - val_accuracy: 0.8428\n",
            "Epoch 112/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9004\n",
            "Epoch 112: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3209 - accuracy: 0.9004 - val_loss: 0.4978 - val_accuracy: 0.8494\n",
            "Epoch 113/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.8963\n",
            "Epoch 113: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3285 - accuracy: 0.8963 - val_loss: 0.5888 - val_accuracy: 0.8258\n",
            "Epoch 114/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.8989\n",
            "Epoch 114: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3217 - accuracy: 0.8989 - val_loss: 0.5275 - val_accuracy: 0.8424\n",
            "Epoch 115/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.9018\n",
            "Epoch 115: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3152 - accuracy: 0.9018 - val_loss: 0.5238 - val_accuracy: 0.8464\n",
            "Epoch 116/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9022\n",
            "Epoch 116: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3095 - accuracy: 0.9022 - val_loss: 0.5258 - val_accuracy: 0.8460\n",
            "Epoch 117/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.9012\n",
            "Epoch 117: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3086 - accuracy: 0.9012 - val_loss: 0.5160 - val_accuracy: 0.8446\n",
            "Epoch 118/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9047\n",
            "Epoch 118: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3078 - accuracy: 0.9047 - val_loss: 0.4886 - val_accuracy: 0.8580\n",
            "Epoch 119/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9043\n",
            "Epoch 119: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3075 - accuracy: 0.9043 - val_loss: 0.6409 - val_accuracy: 0.8242\n",
            "Epoch 120/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9030\n",
            "Epoch 120: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3080 - accuracy: 0.9030 - val_loss: 0.4845 - val_accuracy: 0.8542\n",
            "Epoch 121/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9049\n",
            "Epoch 121: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.3030 - accuracy: 0.9049 - val_loss: 0.6207 - val_accuracy: 0.8254\n",
            "Epoch 122/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.9045\n",
            "Epoch 122: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3085 - accuracy: 0.9045 - val_loss: 0.5560 - val_accuracy: 0.8340\n",
            "Epoch 123/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.9056\n",
            "Epoch 123: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.3025 - accuracy: 0.9056 - val_loss: 0.6035 - val_accuracy: 0.8278\n",
            "Epoch 124/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9052\n",
            "Epoch 124: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.3019 - accuracy: 0.9052 - val_loss: 0.5240 - val_accuracy: 0.8460\n",
            "Epoch 125/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9072\n",
            "Epoch 125: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2996 - accuracy: 0.9072 - val_loss: 0.5145 - val_accuracy: 0.8498\n",
            "Epoch 126/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.9070\n",
            "Epoch 126: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2970 - accuracy: 0.9070 - val_loss: 0.5093 - val_accuracy: 0.8472\n",
            "Epoch 127/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.9057\n",
            "Epoch 127: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2975 - accuracy: 0.9057 - val_loss: 0.5165 - val_accuracy: 0.8454\n",
            "Epoch 128/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.9054\n",
            "Epoch 128: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2970 - accuracy: 0.9054 - val_loss: 0.5290 - val_accuracy: 0.8490\n",
            "Epoch 129/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.9059\n",
            "Epoch 129: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2986 - accuracy: 0.9059 - val_loss: 0.4928 - val_accuracy: 0.8528\n",
            "Epoch 130/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9090\n",
            "Epoch 130: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2941 - accuracy: 0.9090 - val_loss: 0.5154 - val_accuracy: 0.8482\n",
            "Epoch 131/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9091\n",
            "Epoch 131: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2925 - accuracy: 0.9091 - val_loss: 0.5306 - val_accuracy: 0.8504\n",
            "Epoch 132/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9074\n",
            "Epoch 132: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2922 - accuracy: 0.9074 - val_loss: 0.5096 - val_accuracy: 0.8528\n",
            "Epoch 133/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9074\n",
            "Epoch 133: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2942 - accuracy: 0.9074 - val_loss: 0.5313 - val_accuracy: 0.8502\n",
            "Epoch 134/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9105\n",
            "Epoch 134: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2845 - accuracy: 0.9105 - val_loss: 0.4896 - val_accuracy: 0.8566\n",
            "Epoch 135/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9077\n",
            "Epoch 135: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2908 - accuracy: 0.9077 - val_loss: 0.5506 - val_accuracy: 0.8424\n",
            "Epoch 136/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9096\n",
            "Epoch 136: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2886 - accuracy: 0.9096 - val_loss: 0.5827 - val_accuracy: 0.8374\n",
            "Epoch 137/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9120\n",
            "Epoch 137: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2816 - accuracy: 0.9120 - val_loss: 0.4806 - val_accuracy: 0.8560\n",
            "Epoch 138/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9110\n",
            "Epoch 138: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2813 - accuracy: 0.9110 - val_loss: 0.5408 - val_accuracy: 0.8462\n",
            "Epoch 139/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9082\n",
            "Epoch 139: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2904 - accuracy: 0.9082 - val_loss: 0.5258 - val_accuracy: 0.8510\n",
            "Epoch 140/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9079\n",
            "Epoch 140: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2889 - accuracy: 0.9079 - val_loss: 0.5333 - val_accuracy: 0.8474\n",
            "Epoch 141/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.9107\n",
            "Epoch 141: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2843 - accuracy: 0.9107 - val_loss: 0.5107 - val_accuracy: 0.8520\n",
            "Epoch 142/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9106\n",
            "Epoch 142: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2835 - accuracy: 0.9106 - val_loss: 0.4850 - val_accuracy: 0.8604\n",
            "Epoch 143/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9114\n",
            "Epoch 143: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2771 - accuracy: 0.9114 - val_loss: 0.4906 - val_accuracy: 0.8556\n",
            "Epoch 144/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9141\n",
            "Epoch 144: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2721 - accuracy: 0.9141 - val_loss: 0.4801 - val_accuracy: 0.8606\n",
            "Epoch 145/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9145\n",
            "Epoch 145: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2711 - accuracy: 0.9145 - val_loss: 0.5013 - val_accuracy: 0.8502\n",
            "Epoch 146/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9158\n",
            "Epoch 146: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2696 - accuracy: 0.9158 - val_loss: 0.5212 - val_accuracy: 0.8542\n",
            "Epoch 147/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9163\n",
            "Epoch 147: val_loss did not improve from 0.47754\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2668 - accuracy: 0.9163 - val_loss: 0.5010 - val_accuracy: 0.8558\n",
            "Epoch 148/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9164\n",
            "Epoch 148: val_loss improved from 0.47754 to 0.47153, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2657 - accuracy: 0.9164 - val_loss: 0.4715 - val_accuracy: 0.8616\n",
            "Epoch 149/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9165\n",
            "Epoch 149: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2675 - accuracy: 0.9165 - val_loss: 0.5349 - val_accuracy: 0.8500\n",
            "Epoch 150/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9167\n",
            "Epoch 150: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2657 - accuracy: 0.9167 - val_loss: 0.5360 - val_accuracy: 0.8576\n",
            "Epoch 151/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9162\n",
            "Epoch 151: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2638 - accuracy: 0.9162 - val_loss: 0.5424 - val_accuracy: 0.8464\n",
            "Epoch 152/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9156\n",
            "Epoch 152: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2652 - accuracy: 0.9156 - val_loss: 0.4839 - val_accuracy: 0.8668\n",
            "Epoch 153/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9172\n",
            "Epoch 153: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2630 - accuracy: 0.9172 - val_loss: 0.5188 - val_accuracy: 0.8566\n",
            "Epoch 154/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9172\n",
            "Epoch 154: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2639 - accuracy: 0.9172 - val_loss: 0.4928 - val_accuracy: 0.8586\n",
            "Epoch 155/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9190\n",
            "Epoch 155: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2569 - accuracy: 0.9190 - val_loss: 0.5120 - val_accuracy: 0.8540\n",
            "Epoch 156/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.9165\n",
            "Epoch 156: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2644 - accuracy: 0.9165 - val_loss: 0.5584 - val_accuracy: 0.8436\n",
            "Epoch 157/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9200\n",
            "Epoch 157: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2548 - accuracy: 0.9200 - val_loss: 0.4999 - val_accuracy: 0.8562\n",
            "Epoch 158/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9195\n",
            "Epoch 158: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2602 - accuracy: 0.9195 - val_loss: 0.5343 - val_accuracy: 0.8458\n",
            "Epoch 159/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9180\n",
            "Epoch 159: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2585 - accuracy: 0.9180 - val_loss: 0.5036 - val_accuracy: 0.8532\n",
            "Epoch 160/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9197\n",
            "Epoch 160: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2599 - accuracy: 0.9197 - val_loss: 0.4822 - val_accuracy: 0.8610\n",
            "Epoch 161/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9178\n",
            "Epoch 161: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2614 - accuracy: 0.9178 - val_loss: 0.5084 - val_accuracy: 0.8524\n",
            "Epoch 162/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9204\n",
            "Epoch 162: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2543 - accuracy: 0.9204 - val_loss: 0.4893 - val_accuracy: 0.8620\n",
            "Epoch 163/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9205\n",
            "Epoch 163: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2549 - accuracy: 0.9205 - val_loss: 0.5862 - val_accuracy: 0.8408\n",
            "Epoch 164/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9187\n",
            "Epoch 164: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2566 - accuracy: 0.9187 - val_loss: 0.4922 - val_accuracy: 0.8634\n",
            "Epoch 165/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9199\n",
            "Epoch 165: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2533 - accuracy: 0.9199 - val_loss: 0.5143 - val_accuracy: 0.8568\n",
            "Epoch 166/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9198\n",
            "Epoch 166: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2531 - accuracy: 0.9198 - val_loss: 0.5246 - val_accuracy: 0.8512\n",
            "Epoch 167/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9210\n",
            "Epoch 167: val_loss did not improve from 0.47153\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2534 - accuracy: 0.9210 - val_loss: 0.5402 - val_accuracy: 0.8464\n",
            "Epoch 168/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9226\n",
            "Epoch 168: val_loss improved from 0.47153 to 0.46426, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.2498 - accuracy: 0.9226 - val_loss: 0.4643 - val_accuracy: 0.8672\n",
            "Epoch 169/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9218\n",
            "Epoch 169: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2477 - accuracy: 0.9218 - val_loss: 0.5185 - val_accuracy: 0.8550\n",
            "Epoch 170/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9223\n",
            "Epoch 170: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2473 - accuracy: 0.9223 - val_loss: 0.5301 - val_accuracy: 0.8492\n",
            "Epoch 171/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9237\n",
            "Epoch 171: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2455 - accuracy: 0.9237 - val_loss: 0.5076 - val_accuracy: 0.8564\n",
            "Epoch 172/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9244\n",
            "Epoch 172: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2415 - accuracy: 0.9244 - val_loss: 0.4721 - val_accuracy: 0.8602\n",
            "Epoch 173/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9243\n",
            "Epoch 173: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2400 - accuracy: 0.9243 - val_loss: 0.5628 - val_accuracy: 0.8560\n",
            "Epoch 174/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9244\n",
            "Epoch 174: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2392 - accuracy: 0.9244 - val_loss: 0.5265 - val_accuracy: 0.8556\n",
            "Epoch 175/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9249\n",
            "Epoch 175: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2447 - accuracy: 0.9249 - val_loss: 0.5499 - val_accuracy: 0.8504\n",
            "Epoch 176/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9268\n",
            "Epoch 176: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2355 - accuracy: 0.9268 - val_loss: 0.5209 - val_accuracy: 0.8568\n",
            "Epoch 177/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9248\n",
            "Epoch 177: val_loss did not improve from 0.46426\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2381 - accuracy: 0.9248 - val_loss: 0.5585 - val_accuracy: 0.8498\n",
            "Epoch 178/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9269\n",
            "Epoch 178: val_loss improved from 0.46426 to 0.45814, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.2345 - accuracy: 0.9269 - val_loss: 0.4581 - val_accuracy: 0.8714\n",
            "Epoch 179/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9257\n",
            "Epoch 179: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2372 - accuracy: 0.9257 - val_loss: 0.5018 - val_accuracy: 0.8568\n",
            "Epoch 180/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9269\n",
            "Epoch 180: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2339 - accuracy: 0.9269 - val_loss: 0.5126 - val_accuracy: 0.8562\n",
            "Epoch 181/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9268\n",
            "Epoch 181: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2327 - accuracy: 0.9268 - val_loss: 0.5207 - val_accuracy: 0.8570\n",
            "Epoch 182/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9267\n",
            "Epoch 182: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2347 - accuracy: 0.9267 - val_loss: 0.5373 - val_accuracy: 0.8550\n",
            "Epoch 183/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9278\n",
            "Epoch 183: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2321 - accuracy: 0.9278 - val_loss: 0.5077 - val_accuracy: 0.8616\n",
            "Epoch 184/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9264\n",
            "Epoch 184: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2315 - accuracy: 0.9264 - val_loss: 0.5616 - val_accuracy: 0.8512\n",
            "Epoch 185/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9249\n",
            "Epoch 185: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2351 - accuracy: 0.9249 - val_loss: 0.4900 - val_accuracy: 0.8634\n",
            "Epoch 186/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9283\n",
            "Epoch 186: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2330 - accuracy: 0.9283 - val_loss: 0.4761 - val_accuracy: 0.8650\n",
            "Epoch 187/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9295\n",
            "Epoch 187: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2267 - accuracy: 0.9295 - val_loss: 0.4846 - val_accuracy: 0.8610\n",
            "Epoch 188/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9268\n",
            "Epoch 188: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2303 - accuracy: 0.9268 - val_loss: 0.5284 - val_accuracy: 0.8534\n",
            "Epoch 189/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9291\n",
            "Epoch 189: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2267 - accuracy: 0.9291 - val_loss: 0.5435 - val_accuracy: 0.8562\n",
            "Epoch 190/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9273\n",
            "Epoch 190: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2286 - accuracy: 0.9273 - val_loss: 0.4961 - val_accuracy: 0.8628\n",
            "Epoch 191/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9284\n",
            "Epoch 191: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2286 - accuracy: 0.9284 - val_loss: 0.4879 - val_accuracy: 0.8636\n",
            "Epoch 192/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9297\n",
            "Epoch 192: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2240 - accuracy: 0.9297 - val_loss: 0.5461 - val_accuracy: 0.8532\n",
            "Epoch 193/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9292\n",
            "Epoch 193: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2256 - accuracy: 0.9292 - val_loss: 0.5244 - val_accuracy: 0.8580\n",
            "Epoch 194/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9308\n",
            "Epoch 194: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2253 - accuracy: 0.9308 - val_loss: 0.5404 - val_accuracy: 0.8562\n",
            "Epoch 195/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9269\n",
            "Epoch 195: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2267 - accuracy: 0.9269 - val_loss: 0.4839 - val_accuracy: 0.8682\n",
            "Epoch 196/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9303\n",
            "Epoch 196: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2228 - accuracy: 0.9303 - val_loss: 0.5245 - val_accuracy: 0.8542\n",
            "Epoch 197/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9299\n",
            "Epoch 197: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2242 - accuracy: 0.9299 - val_loss: 0.4827 - val_accuracy: 0.8696\n",
            "Epoch 198/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2238 - accuracy: 0.9294\n",
            "Epoch 198: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2238 - accuracy: 0.9294 - val_loss: 0.5186 - val_accuracy: 0.8612\n",
            "Epoch 199/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.9290\n",
            "Epoch 199: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2246 - accuracy: 0.9290 - val_loss: 0.5394 - val_accuracy: 0.8534\n",
            "Epoch 200/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9333\n",
            "Epoch 200: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2131 - accuracy: 0.9333 - val_loss: 0.5186 - val_accuracy: 0.8524\n",
            "Epoch 201/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9326\n",
            "Epoch 201: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2145 - accuracy: 0.9326 - val_loss: 0.5186 - val_accuracy: 0.8622\n",
            "Epoch 202/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9313\n",
            "Epoch 202: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.2182 - accuracy: 0.9313 - val_loss: 0.5011 - val_accuracy: 0.8616\n",
            "Epoch 203/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9322\n",
            "Epoch 203: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2125 - accuracy: 0.9322 - val_loss: 0.5763 - val_accuracy: 0.8382\n",
            "Epoch 204/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9329\n",
            "Epoch 204: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2146 - accuracy: 0.9329 - val_loss: 0.5481 - val_accuracy: 0.8556\n",
            "Epoch 205/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9318\n",
            "Epoch 205: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2161 - accuracy: 0.9318 - val_loss: 0.4863 - val_accuracy: 0.8624\n",
            "Epoch 206/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9319\n",
            "Epoch 206: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2134 - accuracy: 0.9319 - val_loss: 0.4764 - val_accuracy: 0.8664\n",
            "Epoch 207/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9329\n",
            "Epoch 207: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2126 - accuracy: 0.9329 - val_loss: 0.4757 - val_accuracy: 0.8704\n",
            "Epoch 208/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9342\n",
            "Epoch 208: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2106 - accuracy: 0.9342 - val_loss: 0.4801 - val_accuracy: 0.8660\n",
            "Epoch 209/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9350\n",
            "Epoch 209: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2102 - accuracy: 0.9350 - val_loss: 0.5230 - val_accuracy: 0.8590\n",
            "Epoch 210/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9340\n",
            "Epoch 210: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2109 - accuracy: 0.9340 - val_loss: 0.4770 - val_accuracy: 0.8694\n",
            "Epoch 211/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9344\n",
            "Epoch 211: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2072 - accuracy: 0.9344 - val_loss: 0.5757 - val_accuracy: 0.8480\n",
            "Epoch 212/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9331\n",
            "Epoch 212: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.2135 - accuracy: 0.9331 - val_loss: 0.5390 - val_accuracy: 0.8528\n",
            "Epoch 213/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9344\n",
            "Epoch 213: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2111 - accuracy: 0.9344 - val_loss: 0.5154 - val_accuracy: 0.8652\n",
            "Epoch 214/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9352\n",
            "Epoch 214: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2070 - accuracy: 0.9352 - val_loss: 0.5351 - val_accuracy: 0.8558\n",
            "Epoch 215/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9341\n",
            "Epoch 215: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2109 - accuracy: 0.9341 - val_loss: 0.5156 - val_accuracy: 0.8586\n",
            "Epoch 216/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9345\n",
            "Epoch 216: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2086 - accuracy: 0.9345 - val_loss: 0.5185 - val_accuracy: 0.8598\n",
            "Epoch 217/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9350\n",
            "Epoch 217: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2075 - accuracy: 0.9350 - val_loss: 0.5287 - val_accuracy: 0.8622\n",
            "Epoch 218/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2084 - accuracy: 0.9344\n",
            "Epoch 218: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2084 - accuracy: 0.9344 - val_loss: 0.4683 - val_accuracy: 0.8678\n",
            "Epoch 219/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9354\n",
            "Epoch 219: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2054 - accuracy: 0.9354 - val_loss: 0.4805 - val_accuracy: 0.8730\n",
            "Epoch 220/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.9338\n",
            "Epoch 220: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2052 - accuracy: 0.9338 - val_loss: 0.4770 - val_accuracy: 0.8728\n",
            "Epoch 221/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9374\n",
            "Epoch 221: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2019 - accuracy: 0.9374 - val_loss: 0.5021 - val_accuracy: 0.8610\n",
            "Epoch 222/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.9362\n",
            "Epoch 222: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2052 - accuracy: 0.9362 - val_loss: 0.4822 - val_accuracy: 0.8640\n",
            "Epoch 223/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9375\n",
            "Epoch 223: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.2007 - accuracy: 0.9375 - val_loss: 0.5002 - val_accuracy: 0.8650\n",
            "Epoch 224/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9370\n",
            "Epoch 224: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2026 - accuracy: 0.9370 - val_loss: 0.4861 - val_accuracy: 0.8714\n",
            "Epoch 225/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9349\n",
            "Epoch 225: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2064 - accuracy: 0.9349 - val_loss: 0.4710 - val_accuracy: 0.8626\n",
            "Epoch 226/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9336\n",
            "Epoch 226: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2072 - accuracy: 0.9336 - val_loss: 0.5462 - val_accuracy: 0.8532\n",
            "Epoch 227/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9351\n",
            "Epoch 227: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.2055 - accuracy: 0.9351 - val_loss: 0.4958 - val_accuracy: 0.8686\n",
            "Epoch 228/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9401\n",
            "Epoch 228: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1958 - accuracy: 0.9401 - val_loss: 0.4954 - val_accuracy: 0.8650\n",
            "Epoch 229/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9378\n",
            "Epoch 229: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1972 - accuracy: 0.9378 - val_loss: 0.4959 - val_accuracy: 0.8646\n",
            "Epoch 230/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9409\n",
            "Epoch 230: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1925 - accuracy: 0.9409 - val_loss: 0.5328 - val_accuracy: 0.8592\n",
            "Epoch 231/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9383\n",
            "Epoch 231: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1952 - accuracy: 0.9383 - val_loss: 0.4835 - val_accuracy: 0.8674\n",
            "Epoch 232/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9384\n",
            "Epoch 232: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1985 - accuracy: 0.9384 - val_loss: 0.4658 - val_accuracy: 0.8672\n",
            "Epoch 233/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9377\n",
            "Epoch 233: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1946 - accuracy: 0.9377 - val_loss: 0.4951 - val_accuracy: 0.8686\n",
            "Epoch 234/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9395\n",
            "Epoch 234: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1913 - accuracy: 0.9395 - val_loss: 0.5001 - val_accuracy: 0.8656\n",
            "Epoch 235/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9381\n",
            "Epoch 235: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1970 - accuracy: 0.9381 - val_loss: 0.4783 - val_accuracy: 0.8696\n",
            "Epoch 236/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9387\n",
            "Epoch 236: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.1956 - accuracy: 0.9387 - val_loss: 0.4895 - val_accuracy: 0.8626\n",
            "Epoch 237/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9391\n",
            "Epoch 237: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1949 - accuracy: 0.9391 - val_loss: 0.4959 - val_accuracy: 0.8640\n",
            "Epoch 238/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9402\n",
            "Epoch 238: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1918 - accuracy: 0.9402 - val_loss: 0.4860 - val_accuracy: 0.8650\n",
            "Epoch 239/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9425\n",
            "Epoch 239: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1874 - accuracy: 0.9425 - val_loss: 0.4888 - val_accuracy: 0.8724\n",
            "Epoch 240/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9404\n",
            "Epoch 240: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1920 - accuracy: 0.9404 - val_loss: 0.5768 - val_accuracy: 0.8562\n",
            "Epoch 241/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9378\n",
            "Epoch 241: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1936 - accuracy: 0.9378 - val_loss: 0.4667 - val_accuracy: 0.8750\n",
            "Epoch 242/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9426\n",
            "Epoch 242: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1861 - accuracy: 0.9426 - val_loss: 0.5058 - val_accuracy: 0.8638\n",
            "Epoch 243/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9389\n",
            "Epoch 243: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1943 - accuracy: 0.9389 - val_loss: 0.5128 - val_accuracy: 0.8648\n",
            "Epoch 244/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9394\n",
            "Epoch 244: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1930 - accuracy: 0.9394 - val_loss: 0.5195 - val_accuracy: 0.8600\n",
            "Epoch 245/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9406\n",
            "Epoch 245: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1901 - accuracy: 0.9406 - val_loss: 0.5050 - val_accuracy: 0.8652\n",
            "Epoch 246/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9404\n",
            "Epoch 246: val_loss did not improve from 0.45814\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1880 - accuracy: 0.9404 - val_loss: 0.5022 - val_accuracy: 0.8638\n",
            "Epoch 247/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9397\n",
            "Epoch 247: val_loss improved from 0.45814 to 0.45712, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1930 - accuracy: 0.9397 - val_loss: 0.4571 - val_accuracy: 0.8724\n",
            "Epoch 248/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9405\n",
            "Epoch 248: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1878 - accuracy: 0.9405 - val_loss: 0.4920 - val_accuracy: 0.8708\n",
            "Epoch 249/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9407\n",
            "Epoch 249: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1875 - accuracy: 0.9407 - val_loss: 0.5681 - val_accuracy: 0.8516\n",
            "Epoch 250/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9410\n",
            "Epoch 250: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1864 - accuracy: 0.9410 - val_loss: 0.4707 - val_accuracy: 0.8734\n",
            "Epoch 251/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9405\n",
            "Epoch 251: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1898 - accuracy: 0.9405 - val_loss: 0.5022 - val_accuracy: 0.8668\n",
            "Epoch 252/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.9416\n",
            "Epoch 252: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1871 - accuracy: 0.9416 - val_loss: 0.4842 - val_accuracy: 0.8714\n",
            "Epoch 253/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9412\n",
            "Epoch 253: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1880 - accuracy: 0.9412 - val_loss: 0.4849 - val_accuracy: 0.8718\n",
            "Epoch 254/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9417\n",
            "Epoch 254: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1838 - accuracy: 0.9417 - val_loss: 0.5365 - val_accuracy: 0.8606\n",
            "Epoch 255/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.9403\n",
            "Epoch 255: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1895 - accuracy: 0.9403 - val_loss: 0.5175 - val_accuracy: 0.8598\n",
            "Epoch 256/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9425\n",
            "Epoch 256: val_loss did not improve from 0.45712\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1849 - accuracy: 0.9425 - val_loss: 0.5252 - val_accuracy: 0.8618\n",
            "Epoch 257/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9442\n",
            "Epoch 257: val_loss improved from 0.45712 to 0.45576, saving model to best_model_IsaT_6.h5\n",
            "352/352 [==============================] - 38s 108ms/step - loss: 0.1770 - accuracy: 0.9442 - val_loss: 0.4558 - val_accuracy: 0.8752\n",
            "Epoch 258/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9434\n",
            "Epoch 258: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1808 - accuracy: 0.9434 - val_loss: 0.4606 - val_accuracy: 0.8708\n",
            "Epoch 259/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9443\n",
            "Epoch 259: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1791 - accuracy: 0.9443 - val_loss: 0.4847 - val_accuracy: 0.8724\n",
            "Epoch 260/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9429\n",
            "Epoch 260: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1808 - accuracy: 0.9429 - val_loss: 0.5248 - val_accuracy: 0.8608\n",
            "Epoch 261/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9425\n",
            "Epoch 261: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1819 - accuracy: 0.9425 - val_loss: 0.5182 - val_accuracy: 0.8638\n",
            "Epoch 262/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9447\n",
            "Epoch 262: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1776 - accuracy: 0.9447 - val_loss: 0.5036 - val_accuracy: 0.8694\n",
            "Epoch 263/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9430\n",
            "Epoch 263: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1795 - accuracy: 0.9430 - val_loss: 0.4889 - val_accuracy: 0.8704\n",
            "Epoch 264/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9457\n",
            "Epoch 264: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1756 - accuracy: 0.9457 - val_loss: 0.5163 - val_accuracy: 0.8660\n",
            "Epoch 265/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9436\n",
            "Epoch 265: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1813 - accuracy: 0.9436 - val_loss: 0.5272 - val_accuracy: 0.8650\n",
            "Epoch 266/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9457\n",
            "Epoch 266: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1734 - accuracy: 0.9457 - val_loss: 0.5094 - val_accuracy: 0.8674\n",
            "Epoch 267/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9444\n",
            "Epoch 267: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1771 - accuracy: 0.9444 - val_loss: 0.4988 - val_accuracy: 0.8688\n",
            "Epoch 268/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9461\n",
            "Epoch 268: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1746 - accuracy: 0.9461 - val_loss: 0.4966 - val_accuracy: 0.8680\n",
            "Epoch 269/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9449\n",
            "Epoch 269: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1739 - accuracy: 0.9449 - val_loss: 0.5153 - val_accuracy: 0.8676\n",
            "Epoch 270/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9445\n",
            "Epoch 270: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1741 - accuracy: 0.9445 - val_loss: 0.4940 - val_accuracy: 0.8676\n",
            "Epoch 271/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9444\n",
            "Epoch 271: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1768 - accuracy: 0.9444 - val_loss: 0.4754 - val_accuracy: 0.8710\n",
            "Epoch 272/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9455\n",
            "Epoch 272: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1739 - accuracy: 0.9455 - val_loss: 0.4890 - val_accuracy: 0.8692\n",
            "Epoch 273/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9446\n",
            "Epoch 273: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1765 - accuracy: 0.9446 - val_loss: 0.5019 - val_accuracy: 0.8724\n",
            "Epoch 274/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9440\n",
            "Epoch 274: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1770 - accuracy: 0.9440 - val_loss: 0.5156 - val_accuracy: 0.8646\n",
            "Epoch 275/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9459\n",
            "Epoch 275: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1763 - accuracy: 0.9459 - val_loss: 0.4894 - val_accuracy: 0.8726\n",
            "Epoch 276/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9449\n",
            "Epoch 276: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1748 - accuracy: 0.9449 - val_loss: 0.5085 - val_accuracy: 0.8632\n",
            "Epoch 277/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9476\n",
            "Epoch 277: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1684 - accuracy: 0.9476 - val_loss: 0.5053 - val_accuracy: 0.8726\n",
            "Epoch 278/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9455\n",
            "Epoch 278: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1715 - accuracy: 0.9455 - val_loss: 0.5150 - val_accuracy: 0.8642\n",
            "Epoch 279/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9456\n",
            "Epoch 279: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.1718 - accuracy: 0.9456 - val_loss: 0.5111 - val_accuracy: 0.8648\n",
            "Epoch 280/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1751 - accuracy: 0.9457\n",
            "Epoch 280: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1751 - accuracy: 0.9457 - val_loss: 0.5138 - val_accuracy: 0.8660\n",
            "Epoch 281/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9461\n",
            "Epoch 281: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1733 - accuracy: 0.9461 - val_loss: 0.4809 - val_accuracy: 0.8704\n",
            "Epoch 282/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9457\n",
            "Epoch 282: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1723 - accuracy: 0.9457 - val_loss: 0.5192 - val_accuracy: 0.8614\n",
            "Epoch 283/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9480\n",
            "Epoch 283: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1687 - accuracy: 0.9480 - val_loss: 0.5138 - val_accuracy: 0.8728\n",
            "Epoch 284/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9459\n",
            "Epoch 284: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1723 - accuracy: 0.9459 - val_loss: 0.4972 - val_accuracy: 0.8672\n",
            "Epoch 285/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9470\n",
            "Epoch 285: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.1684 - accuracy: 0.9470 - val_loss: 0.4709 - val_accuracy: 0.8726\n",
            "Epoch 286/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9475\n",
            "Epoch 286: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 104ms/step - loss: 0.1654 - accuracy: 0.9475 - val_loss: 0.4918 - val_accuracy: 0.8698\n",
            "Epoch 287/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9473\n",
            "Epoch 287: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1652 - accuracy: 0.9473 - val_loss: 0.4811 - val_accuracy: 0.8752\n",
            "Epoch 288/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9470\n",
            "Epoch 288: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 107ms/step - loss: 0.1683 - accuracy: 0.9470 - val_loss: 0.5330 - val_accuracy: 0.8652\n",
            "Epoch 289/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9471\n",
            "Epoch 289: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1686 - accuracy: 0.9471 - val_loss: 0.4925 - val_accuracy: 0.8696\n",
            "Epoch 290/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9475\n",
            "Epoch 290: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1688 - accuracy: 0.9475 - val_loss: 0.5204 - val_accuracy: 0.8676\n",
            "Epoch 291/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9485\n",
            "Epoch 291: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1628 - accuracy: 0.9485 - val_loss: 0.5033 - val_accuracy: 0.8702\n",
            "Epoch 292/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9495\n",
            "Epoch 292: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 107ms/step - loss: 0.1627 - accuracy: 0.9495 - val_loss: 0.5283 - val_accuracy: 0.8680\n",
            "Epoch 293/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9480\n",
            "Epoch 293: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1641 - accuracy: 0.9480 - val_loss: 0.5088 - val_accuracy: 0.8646\n",
            "Epoch 294/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1603 - accuracy: 0.9500\n",
            "Epoch 294: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1603 - accuracy: 0.9500 - val_loss: 0.5164 - val_accuracy: 0.8674\n",
            "Epoch 295/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9498\n",
            "Epoch 295: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1634 - accuracy: 0.9498 - val_loss: 0.4853 - val_accuracy: 0.8724\n",
            "Epoch 296/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9500\n",
            "Epoch 296: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1600 - accuracy: 0.9500 - val_loss: 0.5031 - val_accuracy: 0.8676\n",
            "Epoch 297/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9497\n",
            "Epoch 297: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 107ms/step - loss: 0.1628 - accuracy: 0.9497 - val_loss: 0.4991 - val_accuracy: 0.8710\n",
            "Epoch 298/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9471\n",
            "Epoch 298: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1671 - accuracy: 0.9471 - val_loss: 0.5020 - val_accuracy: 0.8698\n",
            "Epoch 299/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9501\n",
            "Epoch 299: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1624 - accuracy: 0.9501 - val_loss: 0.5073 - val_accuracy: 0.8690\n",
            "Epoch 300/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9506\n",
            "Epoch 300: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1604 - accuracy: 0.9506 - val_loss: 0.5055 - val_accuracy: 0.8728\n",
            "Epoch 301/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9492\n",
            "Epoch 301: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 107ms/step - loss: 0.1613 - accuracy: 0.9492 - val_loss: 0.5239 - val_accuracy: 0.8678\n",
            "Epoch 302/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9486\n",
            "Epoch 302: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1633 - accuracy: 0.9486 - val_loss: 0.5132 - val_accuracy: 0.8724\n",
            "Epoch 303/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9478\n",
            "Epoch 303: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1641 - accuracy: 0.9478 - val_loss: 0.4892 - val_accuracy: 0.8756\n",
            "Epoch 304/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9506\n",
            "Epoch 304: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1585 - accuracy: 0.9506 - val_loss: 0.4930 - val_accuracy: 0.8722\n",
            "Epoch 305/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9508\n",
            "Epoch 305: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1579 - accuracy: 0.9508 - val_loss: 0.5017 - val_accuracy: 0.8708\n",
            "Epoch 306/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9516\n",
            "Epoch 306: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1553 - accuracy: 0.9516 - val_loss: 0.5068 - val_accuracy: 0.8704\n",
            "Epoch 307/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9488\n",
            "Epoch 307: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1620 - accuracy: 0.9488 - val_loss: 0.5011 - val_accuracy: 0.8712\n",
            "Epoch 308/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9502\n",
            "Epoch 308: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1581 - accuracy: 0.9502 - val_loss: 0.5140 - val_accuracy: 0.8686\n",
            "Epoch 309/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9508\n",
            "Epoch 309: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1601 - accuracy: 0.9508 - val_loss: 0.4933 - val_accuracy: 0.8728\n",
            "Epoch 310/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9485\n",
            "Epoch 310: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1626 - accuracy: 0.9485 - val_loss: 0.4998 - val_accuracy: 0.8702\n",
            "Epoch 311/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9492\n",
            "Epoch 311: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 38s 108ms/step - loss: 0.1611 - accuracy: 0.9492 - val_loss: 0.4795 - val_accuracy: 0.8754\n",
            "Epoch 312/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9497\n",
            "Epoch 312: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1619 - accuracy: 0.9497 - val_loss: 0.4996 - val_accuracy: 0.8672\n",
            "Epoch 313/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9502\n",
            "Epoch 313: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1574 - accuracy: 0.9502 - val_loss: 0.4801 - val_accuracy: 0.8768\n",
            "Epoch 314/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9508\n",
            "Epoch 314: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1554 - accuracy: 0.9508 - val_loss: 0.5090 - val_accuracy: 0.8698\n",
            "Epoch 315/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9511\n",
            "Epoch 315: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1578 - accuracy: 0.9511 - val_loss: 0.4753 - val_accuracy: 0.8732\n",
            "Epoch 316/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9526\n",
            "Epoch 316: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1507 - accuracy: 0.9526 - val_loss: 0.4906 - val_accuracy: 0.8768\n",
            "Epoch 317/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9526\n",
            "Epoch 317: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1524 - accuracy: 0.9526 - val_loss: 0.5256 - val_accuracy: 0.8692\n",
            "Epoch 318/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9518\n",
            "Epoch 318: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1515 - accuracy: 0.9518 - val_loss: 0.5093 - val_accuracy: 0.8726\n",
            "Epoch 319/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9521\n",
            "Epoch 319: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1553 - accuracy: 0.9521 - val_loss: 0.5437 - val_accuracy: 0.8644\n",
            "Epoch 320/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9506\n",
            "Epoch 320: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1553 - accuracy: 0.9506 - val_loss: 0.4797 - val_accuracy: 0.8730\n",
            "Epoch 321/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9525\n",
            "Epoch 321: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1519 - accuracy: 0.9525 - val_loss: 0.5321 - val_accuracy: 0.8698\n",
            "Epoch 322/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9541\n",
            "Epoch 322: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1490 - accuracy: 0.9541 - val_loss: 0.5033 - val_accuracy: 0.8782\n",
            "Epoch 323/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9515\n",
            "Epoch 323: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1569 - accuracy: 0.9515 - val_loss: 0.4901 - val_accuracy: 0.8750\n",
            "Epoch 324/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9517\n",
            "Epoch 324: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1527 - accuracy: 0.9517 - val_loss: 0.5508 - val_accuracy: 0.8646\n",
            "Epoch 325/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9533\n",
            "Epoch 325: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1487 - accuracy: 0.9533 - val_loss: 0.4749 - val_accuracy: 0.8818\n",
            "Epoch 326/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9530\n",
            "Epoch 326: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1503 - accuracy: 0.9530 - val_loss: 0.5272 - val_accuracy: 0.8694\n",
            "Epoch 327/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9520\n",
            "Epoch 327: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1518 - accuracy: 0.9520 - val_loss: 0.4776 - val_accuracy: 0.8768\n",
            "Epoch 328/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9528\n",
            "Epoch 328: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1534 - accuracy: 0.9528 - val_loss: 0.5048 - val_accuracy: 0.8712\n",
            "Epoch 329/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9514\n",
            "Epoch 329: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1552 - accuracy: 0.9514 - val_loss: 0.4704 - val_accuracy: 0.8790\n",
            "Epoch 330/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9522\n",
            "Epoch 330: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1521 - accuracy: 0.9522 - val_loss: 0.5038 - val_accuracy: 0.8730\n",
            "Epoch 331/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9545\n",
            "Epoch 331: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1472 - accuracy: 0.9545 - val_loss: 0.4865 - val_accuracy: 0.8772\n",
            "Epoch 332/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9507\n",
            "Epoch 332: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1554 - accuracy: 0.9507 - val_loss: 0.4835 - val_accuracy: 0.8750\n",
            "Epoch 333/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9536\n",
            "Epoch 333: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1482 - accuracy: 0.9536 - val_loss: 0.5539 - val_accuracy: 0.8624\n",
            "Epoch 334/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9524\n",
            "Epoch 334: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1527 - accuracy: 0.9524 - val_loss: 0.5275 - val_accuracy: 0.8724\n",
            "Epoch 335/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9544\n",
            "Epoch 335: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1508 - accuracy: 0.9544 - val_loss: 0.4687 - val_accuracy: 0.8804\n",
            "Epoch 336/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9518\n",
            "Epoch 336: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 106ms/step - loss: 0.1524 - accuracy: 0.9518 - val_loss: 0.5049 - val_accuracy: 0.8710\n",
            "Epoch 337/1000\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9532\n",
            "Epoch 337: val_loss did not improve from 0.45576\n",
            "352/352 [==============================] - 37s 105ms/step - loss: 0.1500 - accuracy: 0.9532 - val_loss: 0.5057 - val_accuracy: 0.8708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model on the test data"
      ],
      "metadata": {
        "id": "Gq_Geq9gSrNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "9O3XPWfuSqxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fa0196-3acc-47cd-ab7a-dae2182be031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 10ms/step - loss: 0.4608 - accuracy: 0.8782\n",
            "Test Accuracy: 87.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the accuracy and loss performance"
      ],
      "metadata": {
        "id": "soLOqT2HSxnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracy, label='Training Accuracy')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I8mlrVIbSxJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "84bffb73-009f-4860-a7d8-b89e004697d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}